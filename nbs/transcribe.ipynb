{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steph/.conda/envs/transcriber/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from whisperx import load_model, load_audio, load_align_model, align\n",
    "from whisperx.diarize import assign_word_speakers\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyannote.audio import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"../data/podcast/people_i_admire_104_joy_of_maths(1)/tmp/audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# whisperx config\n",
    "batch_size = 16\n",
    "compute_type = \"float16\"\n",
    "language = \"en\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_tmp_dir(audio_file): return Path(audio_file).parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def whisper_transcribe(audio_file, language=\"en\", batch_size=batch_size, compute_type=compute_type, device=\"cuda\", save=True):\n",
    "    model = load_model(\"large-v2\", device, language=language, compute_type=compute_type)\n",
    "    audio = load_audio(audio_file)\n",
    "    transcript = model.transcribe(audio, language=language, batch_size=batch_size)['segments']\n",
    "    if save: \n",
    "        with open(get_tmp_dir(audio_file)/\"transcript-whisper.json\", \"w\") as f: \n",
    "            json.dump(transcript, f, ensure_ascii=False, indent=2)\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "transcript_whisper = whisper_transcribe(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \" It reminds me a little bit in our last conversation where you talked about the mathematics of vibration and how some combinations of sounds naturally sound good together. But it does seem like a little bit more of a stretch when you apply it to literature. Do you think that's fair? I think it's perhaps less\",\n",
       "  'start': 250.923,\n",
       "  'end': 270.565},\n",
       " {'text': \" on the surface. That's one thing. When you get into looking at how various kinds of poetry or literature are put together, it ceases to feel like a stretch. I'll give you an example of a book. There's a book called The Luminaries by Eleanor Catton. It won the Booker Prize in 2013. That book has a mathematical structure underneath it, which is that every chapter is half the length of the one before.\",\n",
       "  'start': 270.734,\n",
       "  'end': 294.713}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_whisper[10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def whisperx_align(segments, audio_file, language=language, device=\"cuda\", save=True):\n",
    "    model, metadata = load_align_model(language_code=language, device=device)\n",
    "    transcript_aligned = align(segments, model, metadata, audio_file, device=device)\n",
    "    if save:\n",
    "        with open(get_tmp_dir(audio_file)/\"transcript-whisperx.json\", \"w\") as f: \n",
    "            json.dump(transcript_aligned, f, ensure_ascii=False, indent=2)\n",
    "    return transcript_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_whisperx = whisperx_align(deepcopy(transcript_whisper), audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['segments', 'word_segments'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'start': 68.125,\n",
       "  'end': 75.05,\n",
       "  'text': ' So, Sarah, you were on the show back in 2021, and that was a conversation that really sticks with me.',\n",
       "  'words': [{'word': 'So,', 'start': 68.125, 'end': 68.245, 'score': 0.84},\n",
       "   {'word': 'Sarah,', 'start': 68.265, 'end': 68.465, 'score': 0.65},\n",
       "   {'word': 'you', 'start': 68.485, 'end': 68.565, 'score': 0.998},\n",
       "   {'word': 'were', 'start': 68.605, 'end': 68.765, 'score': 0.798},\n",
       "   {'word': 'on', 'start': 68.886, 'end': 68.966, 'score': 0.835},\n",
       "   {'word': 'the', 'start': 68.986, 'end': 69.086, 'score': 0.717},\n",
       "   {'word': 'show', 'start': 69.126, 'end': 69.406, 'score': 0.79},\n",
       "   {'word': 'back', 'start': 69.706, 'end': 69.966, 'score': 0.983},\n",
       "   {'word': 'in', 'start': 70.046, 'end': 70.146, 'score': 0.806},\n",
       "   {'word': '2021,'},\n",
       "   {'word': 'and', 'start': 70.707, 'end': 71.467, 'score': 0.65},\n",
       "   {'word': 'that', 'start': 72.208, 'end': 72.328, 'score': 0.912},\n",
       "   {'word': 'was', 'start': 72.348, 'end': 72.448, 'score': 0.777},\n",
       "   {'word': 'a', 'start': 72.468, 'end': 72.508, 'score': 0.484},\n",
       "   {'word': 'conversation', 'start': 72.568, 'end': 73.329, 'score': 0.812},\n",
       "   {'word': 'that', 'start': 73.369, 'end': 73.509, 'score': 0.844},\n",
       "   {'word': 'really', 'start': 73.629, 'end': 73.929, 'score': 0.902},\n",
       "   {'word': 'sticks', 'start': 74.209, 'end': 74.549, 'score': 0.895},\n",
       "   {'word': 'with', 'start': 74.609, 'end': 74.75, 'score': 0.753},\n",
       "   {'word': 'me.', 'start': 74.79, 'end': 74.93, 'score': 0.712}]},\n",
       " {'start': 75.05,\n",
       "  'end': 80.974,\n",
       "  'text': 'We talked about all sorts of things, but especially about the links between music and math.',\n",
       "  'words': [{'word': 'We', 'start': 75.05, 'end': 75.19, 'score': 0.996},\n",
       "   {'word': 'talked', 'start': 75.35, 'end': 75.63, 'score': 0.906},\n",
       "   {'word': 'about', 'start': 75.71, 'end': 75.97, 'score': 0.84},\n",
       "   {'word': 'all', 'start': 76.311, 'end': 76.431, 'score': 0.733},\n",
       "   {'word': 'sorts', 'start': 76.471, 'end': 76.691, 'score': 0.717},\n",
       "   {'word': 'of', 'start': 76.731, 'end': 76.771, 'score': 1.0},\n",
       "   {'word': 'things,', 'start': 76.811, 'end': 77.071, 'score': 0.757},\n",
       "   {'word': 'but', 'start': 77.552, 'end': 77.672, 'score': 0.725},\n",
       "   {'word': 'especially', 'start': 77.712, 'end': 78.192, 'score': 0.808},\n",
       "   {'word': 'about', 'start': 78.372, 'end': 78.532, 'score': 0.815},\n",
       "   {'word': 'the', 'start': 78.552, 'end': 78.632, 'score': 0.828},\n",
       "   {'word': 'links', 'start': 78.672, 'end': 78.892, 'score': 0.894},\n",
       "   {'word': 'between', 'start': 78.932, 'end': 79.233, 'score': 0.845},\n",
       "   {'word': 'music', 'start': 79.293, 'end': 79.553, 'score': 0.891},\n",
       "   {'word': 'and', 'start': 79.593, 'end': 79.673, 'score': 0.835},\n",
       "   {'word': 'math.', 'start': 79.713, 'end': 80.013, 'score': 0.838}]}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(transcript_whisperx.keys())\n",
    "transcript_whisperx['segments'][10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def diarization_to_df(diarization):\n",
    "    df = pd.DataFrame(diarization.itertracks(yield_label=True))\n",
    "    df['start'] = df[0].apply(lambda x: x.start)\n",
    "    df['end'] = df[0].apply(lambda x: x.end)\n",
    "    df.rename(columns={2: \"speaker\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def diarize(audio_file, n_speakers, device=\"cuda\", hf_token=None, save=True):\n",
    "    if hf_token is None: \n",
    "        with open(str(Path.home()/\".huggingface/token\"), \"r\") as f: hf_token = f.readline()\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization@2.1\",\n",
    "        use_auth_token=hf_token\n",
    "    ).to(torch.device(device))\n",
    "    diarization = pipeline(audio_file, num_speakers=n_speakers)\n",
    "    diarization_df = diarization_to_df(diarization)\n",
    "    if save: \n",
    "        diarization_df.to_csv(get_tmp_dir(audio_file)/\"diarization.csv\")\n",
    "    return diarization_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "diarization = diarize(audio_file, n_speakers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>speaker</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ 00:00:05.644 --&gt;  00:00:17.490]</td>\n",
       "      <td>G</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>5.644687</td>\n",
       "      <td>17.490938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ 00:00:18.132 --&gt;  00:00:28.881]</td>\n",
       "      <td>BO</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "      <td>18.132188</td>\n",
       "      <td>28.881563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ 00:00:31.446 --&gt;  00:00:36.002]</td>\n",
       "      <td>A</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>31.446562</td>\n",
       "      <td>36.002813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ 00:00:37.791 --&gt;  00:01:00.505]</td>\n",
       "      <td>H</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>37.791563</td>\n",
       "      <td>60.505313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ 00:01:08.065 --&gt;  00:01:20.063]</td>\n",
       "      <td>I</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>68.065313</td>\n",
       "      <td>80.063438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0   1     speaker      start        end\n",
       "0  [ 00:00:05.644 -->  00:00:17.490]   G  SPEAKER_01   5.644687  17.490938\n",
       "1  [ 00:00:18.132 -->  00:00:28.881]  BO  SPEAKER_03  18.132188  28.881563\n",
       "2  [ 00:00:31.446 -->  00:00:36.002]   A  SPEAKER_00  31.446562  36.002813\n",
       "3  [ 00:00:37.791 -->  00:01:00.505]   H  SPEAKER_01  37.791563  60.505313\n",
       "4  [ 00:01:08.065 -->  00:01:20.063]   I  SPEAKER_01  68.065313  80.063438"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diarization[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def assign_speakers(diarization, transcript_whisperx):\n",
    "    diarized_transcript = assign_word_speakers(\n",
    "        diarization, transcript_whisperx\n",
    "    )\n",
    "    return diarized_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_diarized = assign_speakers(diarization, deepcopy(transcript_whisperx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['segments', 'word_segments'])\n",
      "8956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'start': 5.688,\n",
       " 'end': 13.491,\n",
       " 'text': ' My guest today, Sarah Hart, is the Gresham Professor of Geometry, the first woman to hold that position in its 400-year history.',\n",
       " 'words': [{'word': 'My',\n",
       "   'start': 5.688,\n",
       "   'end': 5.808,\n",
       "   'score': 0.762,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'guest',\n",
       "   'start': 5.828,\n",
       "   'end': 6.008,\n",
       "   'score': 0.524,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'today,',\n",
       "   'start': 6.028,\n",
       "   'end': 6.348,\n",
       "   'score': 0.719,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Sarah',\n",
       "   'start': 6.488,\n",
       "   'end': 6.788,\n",
       "   'score': 0.707,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Hart,',\n",
       "   'start': 6.829,\n",
       "   'end': 7.069,\n",
       "   'score': 0.814,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'is',\n",
       "   'start': 7.349,\n",
       "   'end': 7.449,\n",
       "   'score': 0.641,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'the',\n",
       "   'start': 7.469,\n",
       "   'end': 7.549,\n",
       "   'score': 0.966,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Gresham',\n",
       "   'start': 7.589,\n",
       "   'end': 7.929,\n",
       "   'score': 0.721,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Professor',\n",
       "   'start': 7.969,\n",
       "   'end': 8.349,\n",
       "   'score': 0.922,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'of',\n",
       "   'start': 8.389,\n",
       "   'end': 8.429,\n",
       "   'score': 0.995,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Geometry,',\n",
       "   'start': 8.489,\n",
       "   'end': 9.029,\n",
       "   'score': 0.864,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'the',\n",
       "   'start': 9.43,\n",
       "   'end': 9.53,\n",
       "   'score': 0.897,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'first',\n",
       "   'start': 9.57,\n",
       "   'end': 9.83,\n",
       "   'score': 0.813,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'woman',\n",
       "   'start': 9.89,\n",
       "   'end': 10.11,\n",
       "   'score': 0.877,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'to',\n",
       "   'start': 10.13,\n",
       "   'end': 10.19,\n",
       "   'score': 0.8,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'hold',\n",
       "   'start': 10.23,\n",
       "   'end': 10.41,\n",
       "   'score': 0.734,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'that',\n",
       "   'start': 10.45,\n",
       "   'end': 10.57,\n",
       "   'score': 0.875,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'position',\n",
       "   'start': 10.63,\n",
       "   'end': 11.03,\n",
       "   'score': 0.887,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'in',\n",
       "   'start': 11.25,\n",
       "   'end': 11.33,\n",
       "   'score': 0.846,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'its',\n",
       "   'start': 11.37,\n",
       "   'end': 11.871,\n",
       "   'score': 0.726,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': '400-year',\n",
       "   'start': 11.891,\n",
       "   'end': 12.531,\n",
       "   'score': 0.751,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'history.',\n",
       "   'start': 12.551,\n",
       "   'end': 12.911,\n",
       "   'score': 0.825,\n",
       "   'speaker': 'SPEAKER_01'}],\n",
       " 'speaker': 'SPEAKER_01'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(transcript_diarized.keys())\n",
    "print(len(transcript_diarized['word_segments']))\n",
    "transcript_diarized['segments'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'guest',\n",
       " 'today,',\n",
       " 'Sarah',\n",
       " 'Hart,',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Gresham',\n",
       " 'Professor',\n",
       " 'of']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word['word'] for word in transcript_diarized['word_segments'][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# whisperx has some fields missing, \n",
    "# which we added by setting them to their previous value\n",
    "def add_missing_field(obj, key, prev_value, missing_log=None):\n",
    "    if not key in obj: \n",
    "        obj[key] = prev_value\n",
    "        if missing_log: missing_log[key].append(obj)\n",
    "    prev_value = obj[key]\n",
    "    return obj, prev_value\n",
    "\n",
    "def find_first_speaker(objects):\n",
    "    for obj in objects:\n",
    "        if 'speaker' in obj:\n",
    "            return obj['speaker']\n",
    "\n",
    "def add_missing_segment_values(segments):\n",
    "    prev_speaker = find_first_speaker(segments); prev_start = 0.; prev_end = 0.\n",
    "    missing_log = { \"speaker\": [], \"start\": [], \"end\": [] }\n",
    "    for s in segments:\n",
    "        s, prev_speaker = add_missing_field(s, 'speaker', prev_speaker, missing_log)\n",
    "        for w in s['words']:\n",
    "            w, prev_start = add_missing_field(w, 'start', prev_start, missing_log)\n",
    "            w, prev_end = add_missing_field(w, 'end', prev_end, missing_log)\n",
    "    return segments, missing_log\n",
    "\n",
    "def add_missing_word_values(words):\n",
    "    prev_speaker = find_first_speaker(words); prev_start = 0.; prev_end = 0.\n",
    "    for w in words:\n",
    "        w, prev_speaker = add_missing_field(w, 'speaker', prev_speaker)\n",
    "        w, prev_start = add_missing_field(w, 'start', prev_start)\n",
    "        w, prev_end = add_missing_field(w, 'end', prev_end)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_fixed, missing_values = add_missing_segment_values(deepcopy(transcript_diarized['segments']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['start', 'end', 'text', 'words', 'speaker'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_fixed[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021,', '17', '5', '7', '5.', '5,', '7,', '17,', '17?', '16?', '12', '17', '2013.', '14', '10', '10', '10', '10', '10', '10', '10', '10,', '14', '100', '26', '61.', '273.', '10', '16', '32', '24', '2', '2', '10,', '2', '10', '1,024', '50', '42.', '42.', '2,', '2', '2', '2,', '8.', '2', '2,', '4.', '8.', '4.', '10', '10', '10', '15.', '20', '20', '1597', '2021.', '3,000', '12', '49', '96.']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print([word['word'] for word in missing_values['start']]) # missing values are all numeric\n",
    "print([word['word'] for word in missing_values['speaker']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering speaker numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've found this was causing confusion, to myslef as well as the LLMs, so here we want to avoid the speaker numbers to appear sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['segments', 'word_segments'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_diarized.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPEAKER_00\n",
      "SPEAKER_00\n",
      "SPEAKER_01\n",
      "SPEAKER_01\n",
      "SPEAKER_01\n",
      "SPEAKER_01\n",
      "SPEAKER_02\n",
      "SPEAKER_00\n",
      "SPEAKER_00\n",
      "SPEAKER_00\n"
     ]
    }
   ],
   "source": [
    "unique_speakers = []\n",
    "for speech in transcript_diarized['segments']:\n",
    "    if speech['speaker'] not in unique_speakers:\n",
    "        unique_speakers.append(speech['speaker'])\n",
    "\n",
    "renamed_speakers = {speaker:'SPEAKER_0'+str(i) for i, speaker in enumerate(unique_speakers)}\n",
    "\n",
    "for speech in transcript_diarized['segments']:\n",
    "    speech['speaker'] = renamed_speakers[speech['speaker']]\n",
    "\n",
    "for word in transcript_diarized['word_segments']:\n",
    "    word['speaker'] = renamed_speakers[word['speaker']]\n",
    "\n",
    "for speech in transcript_diarized['segments'][:10]:\n",
    "    print(speech['speaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def order_speakers(transcript_words):\n",
    "\n",
    "    # cannot use set as we need to preserve the order of which they appear\n",
    "    unique_speakers = []\n",
    "    for speech in transcript_words:\n",
    "        if speech['speaker'] not in unique_speakers:\n",
    "            unique_speakers.append(speech['speaker'])\n",
    "    \n",
    "    rename_mapping = {speaker:'SPEAKER_0'+str(i) for i, speaker in enumerate(unique_speakers)}\n",
    "\n",
    "    for word in transcript_words:\n",
    "        word['speaker'] = rename_mapping[word['speaker']]\n",
    "\n",
    "    return transcript_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_transcript = order_speakers(add_missing_word_values(transcript_diarized['word_segments']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def process_transcript(transcript):\n",
    "    return order_speakers(\n",
    "        add_missing_word_values(\n",
    "            transcript['word_segments'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['segments', 'word_segments'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_diarized.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_processed = process_transcript(deepcopy(transcript_diarized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'My', 'start': 5.688, 'end': 5.808, 'score': 0.762, 'speaker': 'SPEAKER_00'}\n"
     ]
    }
   ],
   "source": [
    "print(transcript_processed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(get_tmp_dir(audio_file)/'transcript.json', \"w\", encoding='utf8') as f:\n",
    "    json.dump(transcript_processed, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def transcribe(audio_file, n_speakers, device=\"cuda\", save=True):\n",
    "    transcript_whisper = whisper_transcribe(audio_file=audio_file, device=device)\n",
    "    transcript_aligned = whisperx_align(transcript_whisper, audio_file, device=device)\n",
    "    diarization = diarize(audio_file, n_speakers, device)\n",
    "    transcript_diarized = assign_speakers(diarization, transcript_aligned)\n",
    "    transcript_processed = process_transcript(transcript_diarized)\n",
    "    if save:\n",
    "        with open(get_tmp_dir(audio_file)/'transcript.json', \"w\") as f:\n",
    "            json.dump(transcript_processed, f, ensure_ascii=False, indent=2)\n",
    "    return transcript_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
