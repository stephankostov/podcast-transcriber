{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from whisperx import load_model, load_audio, load_align_model, align\n",
    "from whisperx.diarize import assign_word_speakers\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyannote.audio import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"../data/podcasts/lex_ai_stephen_wolfram_1/tmp/audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# whisperx config\n",
    "batch_size = 16\n",
    "compute_type = \"float16\"\n",
    "language = \"en\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_tmp_dir(audio_file): return Path(audio_file).parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def whisper_transcribe(audio_file, language=\"en\", batch_size=batch_size, compute_type=compute_type, device=\"cuda\", save=True):\n",
    "    model = load_model(\"large-v2\", device, language=language, compute_type=compute_type)\n",
    "    audio = load_audio(audio_file)\n",
    "    transcript = model.transcribe(audio, language=language, batch_size=batch_size)['segments']\n",
    "    if save: \n",
    "        with open(get_tmp_dir(audio_file)/\"transcript-whisper.json\", \"w\") as f: \n",
    "            json.dump(transcript, f, ensure_ascii=False, indent=2)\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "transcript_whisper = whisper_transcribe(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': ' You and your son Christopher helped create the alien language in the movie Arrival. So let me ask maybe a bit of a crazy question, but if aliens were to visit us on earth, do you think we would be able to find a common language? Well,',\n",
       "  'start': 274.21,\n",
       "  'end': 290.326},\n",
       " {'text': \" By the time we're saying aliens are visiting us, we've already prejudiced the whole story. Because the concept of an alien actually visiting, so to speak, we already know they're kind of things that make sense to talk about visiting. So we already know they exist in the same kind of physical setup that we do. It's not just radio signals, it's an actual thing that shows up and so on.\",\n",
       "  'start': 290.63,\n",
       "  'end': 318.203}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_whisper[10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def whisperx_align(segments, audio_file, language=language, device=\"cuda\", save=True):\n",
    "    model, metadata = load_align_model(language_code=language, device=device)\n",
    "    transcript_aligned = align(segments, model, metadata, audio_file, device=device)\n",
    "    if save:\n",
    "        with open(get_tmp_dir(audio_file)/\"transcript-whisperx.json\", \"w\") as f: \n",
    "            json.dump(transcript_aligned, f, ensure_ascii=False, indent=2)\n",
    "    return transcript_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_whisperx = whisperx_align(deepcopy(transcript_whisper), audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['segments', 'word_segments'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'start': 92.728,\n",
       "  'end': 100.255,\n",
       "  'text': 'We now agreed to talk again, probably multiple times in the near future, so this is round one, and stay tuned for round two soon.',\n",
       "  'words': [{'word': 'We', 'start': 92.728, 'end': 92.828, 'score': 0.966},\n",
       "   {'word': 'now', 'start': 92.888, 'end': 93.068, 'score': 0.752},\n",
       "   {'word': 'agreed', 'start': 93.148, 'end': 93.509, 'score': 0.706},\n",
       "   {'word': 'to', 'start': 93.569, 'end': 93.629, 'score': 1.0},\n",
       "   {'word': 'talk', 'start': 93.689, 'end': 93.889, 'score': 0.936},\n",
       "   {'word': 'again,', 'start': 93.969, 'end': 94.229, 'score': 0.844},\n",
       "   {'word': 'probably', 'start': 94.47, 'end': 94.85, 'score': 0.84},\n",
       "   {'word': 'multiple', 'start': 94.91, 'end': 95.27, 'score': 0.866},\n",
       "   {'word': 'times', 'start': 95.31, 'end': 95.591, 'score': 0.802},\n",
       "   {'word': 'in', 'start': 95.631, 'end': 95.691, 'score': 0.736},\n",
       "   {'word': 'the', 'start': 95.711, 'end': 95.771, 'score': 0.955},\n",
       "   {'word': 'near', 'start': 95.831, 'end': 95.991, 'score': 0.826},\n",
       "   {'word': 'future,', 'start': 96.051, 'end': 96.431, 'score': 0.839},\n",
       "   {'word': 'so', 'start': 96.451, 'end': 96.491, 'score': 0.0},\n",
       "   {'word': 'this', 'start': 97.112, 'end': 97.252, 'score': 0.89},\n",
       "   {'word': 'is', 'start': 97.332, 'end': 97.392, 'score': 0.868},\n",
       "   {'word': 'round', 'start': 97.452, 'end': 97.713, 'score': 0.68},\n",
       "   {'word': 'one,', 'start': 97.853, 'end': 97.993, 'score': 0.513},\n",
       "   {'word': 'and', 'start': 98.013, 'end': 98.814, 'score': 0.531},\n",
       "   {'word': 'stay', 'start': 98.854, 'end': 99.094, 'score': 0.794},\n",
       "   {'word': 'tuned', 'start': 99.114, 'end': 99.394, 'score': 0.824},\n",
       "   {'word': 'for', 'start': 99.434, 'end': 99.534, 'score': 0.995},\n",
       "   {'word': 'round', 'start': 99.574, 'end': 99.794, 'score': 0.881},\n",
       "   {'word': 'two', 'start': 99.834, 'end': 100.035, 'score': 0.764},\n",
       "   {'word': 'soon.', 'start': 100.075, 'end': 100.255, 'score': 0.948}]},\n",
       " {'start': 101.737,\n",
       "  'end': 103.839,\n",
       "  'text': ' This is the Artificial Intelligence Podcast.',\n",
       "  'words': [{'word': 'This', 'start': 101.737, 'end': 101.897, 'score': 0.972},\n",
       "   {'word': 'is', 'start': 102.017, 'end': 102.177, 'score': 0.746},\n",
       "   {'word': 'the', 'start': 102.258, 'end': 102.358, 'score': 0.912},\n",
       "   {'word': 'Artificial', 'start': 102.418, 'end': 102.818, 'score': 0.876},\n",
       "   {'word': 'Intelligence', 'start': 102.858, 'end': 103.258, 'score': 0.94},\n",
       "   {'word': 'Podcast.', 'start': 103.299, 'end': 103.819, 'score': 0.894}]}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(transcript_whisperx.keys())\n",
    "transcript_whisperx['segments'][10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def diarization_to_df(diarization):\n",
    "    df = pd.DataFrame(diarization.itertracks(yield_label=True))\n",
    "    df['start'] = df[0].apply(lambda x: x.start)\n",
    "    df['end'] = df[0].apply(lambda x: x.end)\n",
    "    df.rename(columns={2: \"speaker\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def diarize(audio_file, n_speakers, device=\"cuda\", hf_token=None, save=True):\n",
    "    if hf_token is None: \n",
    "        with open(str(Path.home()/\".huggingface/token\"), \"r\") as f: hf_token = f.readline()\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization@2.1\",\n",
    "        use_auth_token=hf_token\n",
    "    ).to(torch.device(device))\n",
    "    diarization = pipeline(audio_file, num_speakers=n_speakers)\n",
    "    diarization_df = diarization_to_df(diarization)\n",
    "    if save: \n",
    "        diarization_df.to_csv(get_tmp_dir(audio_file)/\"diarization.csv\")\n",
    "    return diarization_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "diarization = diarize(audio_file, n_speakers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>speaker</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ 00:00:00.497 --&gt;  00:00:33.370]</td>\n",
       "      <td>HG</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>0.497812</td>\n",
       "      <td>33.370313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ 00:00:34.534 --&gt;  00:01:01.484]</td>\n",
       "      <td>HH</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>34.534688</td>\n",
       "      <td>61.484063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ 00:01:02.479 --&gt;  00:01:22.425]</td>\n",
       "      <td>HI</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>62.479688</td>\n",
       "      <td>82.425938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ 00:01:23.050 --&gt;  00:01:32.027]</td>\n",
       "      <td>HJ</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>83.050313</td>\n",
       "      <td>92.027812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ 00:01:32.652 --&gt;  00:01:40.465]</td>\n",
       "      <td>HK</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>92.652187</td>\n",
       "      <td>100.465312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0   1     speaker      start         end\n",
       "0  [ 00:00:00.497 -->  00:00:33.370]  HG  SPEAKER_01   0.497812   33.370313\n",
       "1  [ 00:00:34.534 -->  00:01:01.484]  HH  SPEAKER_01  34.534688   61.484063\n",
       "2  [ 00:01:02.479 -->  00:01:22.425]  HI  SPEAKER_01  62.479688   82.425938\n",
       "3  [ 00:01:23.050 -->  00:01:32.027]  HJ  SPEAKER_01  83.050313   92.027812\n",
       "4  [ 00:01:32.652 -->  00:01:40.465]  HK  SPEAKER_01  92.652187  100.465312"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diarization[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def assign_speakers(diarization, transcript_whisperx):\n",
    "    diarized_transcript = assign_word_speakers(\n",
    "        diarization, transcript_whisperx\n",
    "    )\n",
    "    return diarized_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_diarized = assign_speakers(diarization, deepcopy(transcript_whisperx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['segments', 'word_segments'])\n",
      "32747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'start': 0.128,\n",
       " 'end': 16.34,\n",
       " 'text': ' The following is a conversation with Stephen Wolfram, a computer scientist, mathematician, and theoretical physicist, who is the founder and CEO of Wolfram Research, a company behind Mathematica, Wolfram Alpha, Wolfram Language, and the new Wolfram Physics Project.',\n",
       " 'words': [{'word': 'The', 'start': 0.128, 'end': 0.208, 'score': 0.947},\n",
       "  {'word': 'following',\n",
       "   'start': 0.248,\n",
       "   'end': 0.568,\n",
       "   'score': 0.846,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'is',\n",
       "   'start': 0.608,\n",
       "   'end': 0.668,\n",
       "   'score': 0.785,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'a',\n",
       "   'start': 0.709,\n",
       "   'end': 0.749,\n",
       "   'score': 0.491,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'conversation',\n",
       "   'start': 0.789,\n",
       "   'end': 1.369,\n",
       "   'score': 0.917,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'with',\n",
       "   'start': 1.429,\n",
       "   'end': 1.529,\n",
       "   'score': 0.477,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Stephen',\n",
       "   'start': 1.549,\n",
       "   'end': 1.849,\n",
       "   'score': 0.911,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Wolfram,',\n",
       "   'start': 1.889,\n",
       "   'end': 2.31,\n",
       "   'score': 0.792,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'a',\n",
       "   'start': 2.75,\n",
       "   'end': 2.81,\n",
       "   'score': 0.682,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'computer',\n",
       "   'start': 2.83,\n",
       "   'end': 3.17,\n",
       "   'score': 0.888,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'scientist,',\n",
       "   'start': 3.19,\n",
       "   'end': 3.711,\n",
       "   'score': 0.885,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'mathematician,',\n",
       "   'start': 3.811,\n",
       "   'end': 4.511,\n",
       "   'score': 0.876,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'and',\n",
       "   'start': 4.671,\n",
       "   'end': 4.731,\n",
       "   'score': 0.685,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'theoretical',\n",
       "   'start': 4.772,\n",
       "   'end': 5.212,\n",
       "   'score': 0.828,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'physicist,',\n",
       "   'start': 5.252,\n",
       "   'end': 5.792,\n",
       "   'score': 0.948,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'who',\n",
       "   'start': 6.253,\n",
       "   'end': 6.353,\n",
       "   'score': 0.93,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'is',\n",
       "   'start': 6.393,\n",
       "   'end': 6.473,\n",
       "   'score': 0.813,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'the',\n",
       "   'start': 6.513,\n",
       "   'end': 6.593,\n",
       "   'score': 0.846,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'founder',\n",
       "   'start': 6.673,\n",
       "   'end': 7.113,\n",
       "   'score': 0.812,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'and',\n",
       "   'start': 7.213,\n",
       "   'end': 7.293,\n",
       "   'score': 0.879,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'CEO',\n",
       "   'start': 7.333,\n",
       "   'end': 7.934,\n",
       "   'score': 0.882,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'of',\n",
       "   'start': 8.374,\n",
       "   'end': 8.454,\n",
       "   'score': 0.82,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Wolfram',\n",
       "   'start': 8.494,\n",
       "   'end': 8.895,\n",
       "   'score': 0.879,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Research,',\n",
       "   'start': 8.955,\n",
       "   'end': 9.395,\n",
       "   'score': 0.79,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'a',\n",
       "   'start': 9.815,\n",
       "   'end': 9.835,\n",
       "   'score': 0.999,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'company',\n",
       "   'start': 9.895,\n",
       "   'end': 10.276,\n",
       "   'score': 0.895,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'behind',\n",
       "   'start': 10.336,\n",
       "   'end': 10.636,\n",
       "   'score': 0.835,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Mathematica,',\n",
       "   'start': 10.676,\n",
       "   'end': 11.336,\n",
       "   'score': 0.924,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Wolfram',\n",
       "   'start': 11.637,\n",
       "   'end': 11.957,\n",
       "   'score': 0.726,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Alpha,',\n",
       "   'start': 12.077,\n",
       "   'end': 12.397,\n",
       "   'score': 0.949,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Wolfram',\n",
       "   'start': 12.537,\n",
       "   'end': 12.878,\n",
       "   'score': 0.658,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Language,',\n",
       "   'start': 12.898,\n",
       "   'end': 13.298,\n",
       "   'score': 0.86,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'and',\n",
       "   'start': 13.598,\n",
       "   'end': 13.698,\n",
       "   'score': 0.963,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'the',\n",
       "   'start': 13.738,\n",
       "   'end': 13.818,\n",
       "   'score': 0.877,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'new',\n",
       "   'start': 13.878,\n",
       "   'end': 14.078,\n",
       "   'score': 0.838,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Wolfram',\n",
       "   'start': 14.339,\n",
       "   'end': 14.699,\n",
       "   'score': 0.74,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Physics',\n",
       "   'start': 14.739,\n",
       "   'end': 15.079,\n",
       "   'score': 0.863,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'word': 'Project.',\n",
       "   'start': 15.119,\n",
       "   'end': 15.539,\n",
       "   'score': 0.801,\n",
       "   'speaker': 'SPEAKER_01'}],\n",
       " 'speaker': 'SPEAKER_01'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(transcript_diarized.keys())\n",
    "print(len(transcript_diarized['word_segments']))\n",
    "transcript_diarized['segments'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'following',\n",
       " 'is',\n",
       " 'a',\n",
       " 'conversation',\n",
       " 'with',\n",
       " 'Stephen',\n",
       " 'Wolfram,',\n",
       " 'a',\n",
       " 'computer']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word['word'] for word in transcript_diarized['word_segments'][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# whisperx has some fields missing, \n",
    "# which we added by setting them to their previous value\n",
    "def add_missing_field(obj, key, prev_value, missing_log=None):\n",
    "    if not key in obj: \n",
    "        obj[key] = prev_value\n",
    "        if missing_log: missing_log[key].append(obj)\n",
    "    prev_value = obj[key]\n",
    "    return obj, prev_value\n",
    "\n",
    "def find_first_speaker(objects):\n",
    "    for obj in objects:\n",
    "        if 'speaker' in obj:\n",
    "            return obj['speaker']\n",
    "\n",
    "def add_missing_segment_values(segments):\n",
    "    prev_speaker = find_first_speaker(segments); prev_start = 0.; prev_end = 0.\n",
    "    missing_log = { \"speaker\": [], \"start\": [], \"end\": [] }\n",
    "    for s in segments:\n",
    "        s, prev_speaker = add_missing_field(s, 'speaker', prev_speaker, missing_log)\n",
    "        for w in s['words']:\n",
    "            w, prev_start = add_missing_field(w, 'start', prev_start, missing_log)\n",
    "            w, prev_end = add_missing_field(w, 'end', prev_end, missing_log)\n",
    "    return segments, missing_log\n",
    "\n",
    "def add_missing_word_values(words):\n",
    "    prev_speaker = find_first_speaker(words); prev_start = 0.; prev_end = 0.\n",
    "    for w in words:\n",
    "        w, prev_speaker = add_missing_field(w, 'speaker', prev_speaker)\n",
    "        w, prev_start = add_missing_field(w, 'start', prev_start)\n",
    "        w, prev_end = add_missing_field(w, 'end', prev_end)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_fixed, missing_values = add_missing_segment_values(deepcopy(transcript_diarized['segments']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['start', 'end', 'text', 'words', 'speaker'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_fixed[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019,', '$1.', '$10,', '$10', '2001', '30', '30', '1,000', '42,', '10', '150', '1900,', '1931', '1936,', '3785.', '50,000', '14.6', '30', '100', '100', '1915,', '100', '200', '200', '300', '10', '20', '1980,', '81,', '30', '30', '30', '100', '2002,', '1200', '1,200', '300', '30', '3.1415926,', '30,', '30,', '30', '30', '30.', '400', '30,', '1984,', '300', '30', '30', '30', '30,', '30', '$30,000', '30,', '$30,000', '10,000', '30', '30.', '30', '30.', '30', '2007', '30,', '400', '1988.', '6,000', '10', '6,000.', '6,000,', '6,000,', '10', '10', '10.', '8%', '57', '8%,', '8%', '89%', '6,000.', '10,', '10', '10', '10', '300', '12,', '60', '33', '10', '30', '20,', '30,', '50', '300', '50', '30', '1%,', '10,', '20%', '500']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print([word['word'] for word in missing_values['start']]) # missing values are all numeric\n",
    "print([word['word'] for word in missing_values['speaker']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def process_transcript(transcript):\n",
    "    return {\n",
    "        'segments': add_missing_segment_values(transcript['segments']),\n",
    "        'words': add_missing_word_values(transcript['word_segments'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['segments', 'word_segments'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_diarized.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_processed = process_transcript(deepcopy(transcript_diarized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['segments', 'words'])\n"
     ]
    }
   ],
   "source": [
    "print(transcript_processed.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(get_tmp_dir(audio_file)/'transcript.json', \"w\", encoding='utf8') as f:\n",
    "    json.dump(transcript_processed, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def transcribe(audio_file, n_speakers, device=\"cuda\", save=True):\n",
    "    transcript_whisper = whisper_transcribe(audio_file=audio_file, device=device)\n",
    "    transcript_aligned = whisperx_align(transcript_whisper, audio_file, device=device)\n",
    "    diarization = diarize(audio_file, n_speakers, device)\n",
    "    transcript_diarized = assign_speakers(diarization, transcript_aligned)\n",
    "    transcript_processed = process_transcript(transcript_diarized)\n",
    "    if save:\n",
    "        with open(get_tmp_dir(audio_file)/'transcript.json', \"w\") as f:\n",
    "            json.dump(transcript_processed, f, ensure_ascii=False, indent=2)\n",
    "    return transcript_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
