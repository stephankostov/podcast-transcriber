{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp summarise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarisation\n",
    "\n",
    "> Module for creating chunks of the transcript, for both paragraphs and topics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aims"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above all what is required is a title summary of all our topics for clear navigation of the transcript. \n",
    "\n",
    "A secondary aim would be to also provide summaries for these, to allow readers to quickly understand the epsiode's content."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately this is a summarisation problem, providing the summary as a title and possibly a paragraph description as well.\n",
    "\n",
    "With the introduction of LLMs, there is a clear approach of doing this. It's a method that both has exceptional capabilities for both understanding the text, and also of outputting the contents as desired. These models have been extensively trained on summarisation tasks so they are beyond capable of the task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How will we feed the model information, and instruct it to get an appropriate output. The best way to do this through first impressions is by using delimiters. Both the transcript text and outputs can be delimited through section titles. Before the OpenAI API was released I remember reading in their documents that they recommended separating variables either through triple backquotes or triple hashtags. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputting Transcript"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way for the model to clearly recognise the text that it needs to summarise. This can be done through pasting under a section heading."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require the output to be parseable (backup: use LLMs to parse the appropriate sections). The best way to keep this consistent is to provide examples, and provide a set format for the output to be written in again using section headers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While LLMs will provide excellent results, they are considerably expensive to run. Is there any way we can make this cheaper/easier to run?\n",
    "\n",
    "#### Combining Titles & Summary\n",
    "\n",
    "Ideally, if we can get the the LLM to output reliably, it'd be most efficient to get it to output a title, and a summary in one prompt. Alternatively, we could make use of two-shot prompting, where the previous context would still be held in the model's attention, and a simple request to summarise it would be sufficient. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are required to consider the model size, as they are restricted by my GPU's memory (24G).\n",
    "\n",
    "The introduction to Llama 2 has made this much more simple, as it comes with an already fine-tuned chat model. The choice of models then comes down to two factors: \n",
    "- parameter count\n",
    "- floating point precision\n",
    "\n",
    "A higher number of parameters would allow for more intricate understanding and output possibility, whereas floating point precision would allow for greater stability. It's best to get as much use as possible out of the GPU in this situation, so the choice here is between: \n",
    "\n",
    "model | precision | memory\n",
    "--- | --- | ---\n",
    "llama-chat-7B | FP16 | 7*2=14GB\n",
    "llama-chat-13B | FP8 | 13*1=13GB\n",
    "\n",
    "Since the task at hand isn't that complicated - it doesn't require lots of logic, and doesn't require large degrees of accuracy, it might be best to use the 7B model at a more stable FP16. This though depends on how stable quanitized models are. It should also be noted that this 'stability' is probably more important during model training. Since I'm just using it for inference, the model weights are already known and the decrease in accuracy of them shouldn't effect the output too drastically. The research on this doesn't seem too conclusive as of yet, so its worth investigating empirically.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there's a summary of all the topics, we could use the 'Map Reduce' method on this to obtain the full summary. This is simply combining and summarising all of the individual topic summaries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steph/.conda/envs/test1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to /home/steph/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transcriber.group import group_paragraphs_text\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/podcast/practical_ai_236_tech_stack/tmp/transcript-grouped.json\") as f:\n",
    "    transcript = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_llm_pipeline(model=\"meta-llama/Llama-2-13b-chat-hf\", cache_dir=None, **pipeline_args): \n",
    "    \n",
    "    repo_branch = \"main\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, revision=repo_branch, cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model, revision=repo_branch, cache_dir=cache_dir, load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "\n",
    "    pipe = pipeline(\n",
    "        model=model, tokenizer=tokenizer,\n",
    "        return_full_text=False,  \n",
    "        task='text-generation',\n",
    "        # -- model hyperparameters --\n",
    "        temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "        top_p=0.15,  # select from top tokens whose probability add up to this value\n",
    "        top_k=0,  # select from top 0 tokens (because zero, relies on top_p)\n",
    "        max_new_tokens=400, \n",
    "        repetition_penalty=1.2,  \n",
    "        **pipeline_args\n",
    "    )\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.55s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = load_llm_pipeline(cache_dir = \"/home/steph/.cache/huggingface/os_models\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Titling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would it be best to format the topics as a speaker segmented transcript, or simply as plain text? I think this depends on whether the model would get confused or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_base_prompt(topic_text):\n",
    "    return f\"\"\"\n",
    "\n",
    "Your answer should be displayed in the following format:\n",
    "\n",
    "###SECTION###\n",
    "SPEAKER_00: example text\n",
    "SPEAKER_01: example text\n",
    "\n",
    "###SECTION TITLE###\n",
    "example concise title\n",
    "\n",
    "###SECTION SUMMARY###\n",
    "example concise summary paragraph\n",
    "\n",
    "Now give real titles and summaries for the below:\n",
    "\n",
    "###SECTION###\n",
    "{topic_text}\n",
    "\n",
    "###SECTION TITLE###\n",
    "\"\"\"\n",
    "\n",
    "def get_intro_prompt(topic_text):\n",
    "    return \"For the following podcast introduction section, give it a title followed by a summary. Both the title and summary should be separated into their own section under the headers delimited by triple hashtags.\" + get_base_prompt(topic_text)\n",
    "\n",
    "def get_standard_prompt(topic_text):\n",
    "    return \"You are an AI text summariser who for legal reasons absolutely cannot mention any personal names of the people in the text.\\n\\nFor the following podcast section, give it a title followed by a summary. Both the title and summary should be separated into their own section under the headers delimited by triple hashtags (###).\" + get_base_prompt(topic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_speech_text(topic): return '\\n'.join([speech['label'] + \": \" + speech['text'] for speech in topic['groups']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_summary(summary):\n",
    "    summary = summary.replace(\"'\",'\"')\n",
    "    if summary.startswith('\"') and summary.endswith('\"') and '\"' not in summary[1:-2]:\n",
    "        summary = summary[1:-2]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_summary(llm_summary):\n",
    "    split = llm_summary.split(\"###\")\n",
    "    if len(split) == 3:\n",
    "        return {\n",
    "            'title': format_summary(split[0].strip()),\n",
    "            'summary': format_summary(split[2].strip()),\n",
    "            'summary_unparsed': llm_summary\n",
    "        }\n",
    "    else:\n",
    "        return {'title': \"\", \n",
    "                'summary': \"\", \n",
    "                'summary_unparsed': llm_summary\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_topic_summary_prompt(topic):\n",
    "    speech_text = format_speech_text(topic)\n",
    "    if topic['label'] == 0:\n",
    "        prompt = get_intro_prompt(speech_text)\n",
    "    else:\n",
    "        prompt = get_standard_prompt(speech_text)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def summarise_topics(transcript, llm):\n",
    "    for i, topic in enumerate(transcript):\n",
    "        prompt = get_topic_summary_prompt(topic)\n",
    "        summary = parse_summary(\n",
    "            llm(prompt)[0]['generated_text']\n",
    "        )\n",
    "        topic.update(summary)\n",
    "        torch.cuda.empty_cache()\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steph/.conda/envs/test1/lib/python3.10/site-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "summarised_topics = summarise_topics(transcript, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Understanding Large Language Models and Their Role in Generative AI Applications',\n",
       "  'summary': 'In this episode of Practical AI, hosts Daniel Whitenack and Chris Benson explore the concept of large language models (LLMs) and their role in generative AI applications. They discuss the differences between LLMs and other types of AI models, and examine the various components that make up the LLM ecosystem. Additionally, they touch on the idea that the model itself is not the application, but rather a tool that can be used to create value.',\n",
       "  'summary_unparsed': 'Understanding Large Language Models and Their Role in Generative AI Applications\\n\\n###SECTION SUMMARY###\\nIn this episode of Practical AI, hosts Daniel Whitenack and Chris Benson explore the concept of large language models (LLMs) and their role in generative AI applications. They discuss the differences between LLMs and other types of AI models, and examine the various components that make up the LLM ecosystem. Additionally, they touch on the idea that the model itself is not the application, but rather a tool that can be used to create value.'},\n",
       " {'title': 'Exploring the Generative AI App Stack: Playgrounds and Hosting Providers',\n",
       "  'summary': 'In this section, we discuss two key categories within the generative AI app stack: playgrounds and hosting providers. We explore the different types of playgrounds available, including those offered by cloud providers and commercial entities, and their focus on showcasing their products and services. We also examine the characteristics of playgrounds, such as interactivity and experimental nature, and how they differ from traditional demo interfaces. Additionally, we touch on app hosting, which is not unique to generative AI but is still an important aspect of the app stack, and the variety of hosting providers available, including Vercel and Amazon Web Services.',\n",
       "  'summary_unparsed': 'Exploring the Generative AI App Stack: Playgrounds and Hosting Providers\\n\\n###SECTION SUMMARY###\\nIn this section, we discuss two key categories within the generative AI app stack: playgrounds and hosting providers. We explore the different types of playgrounds available, including those offered by cloud providers and commercial entities, and their focus on showcasing their products and services. We also examine the characteristics of playgrounds, such as interactivity and experimental nature, and how they differ from traditional demo interfaces. Additionally, we touch on app hosting, which is not unique to generative AI but is still an important aspect of the app stack, and the variety of hosting providers available, including Vercel and Amazon Web Services.'},\n",
       " {'title': 'The Emerging Generative AI App Stack: Orchestration and Convenience Layer',\n",
       "  'summary': 'This section discusses the \"orchestration\" layer in the emerging generative AI app stack, focusing on the convenience and tooling aspects of making LLMs usable in practical applications. Speakers highlight the diversity of tools and approaches within this space, including Python/DIY, LangChain, and chat GPT. They emphasize the importance of automation and template-based functionality for simplifying the process of working with LLMs.',\n",
       "  'summary_unparsed': '\"The Emerging Generative AI App Stack: Orchestration and Convenience Layers\"\\n\\n###SECTION SUMMARY###\\nThis section discusses the \"orchestration\" layer in the emerging generative AI app stack, focusing on the convenience and tooling aspects of making LLMs usable in practical applications. Speakers highlight the diversity of tools and approaches within this space, including Python/DIY, LangChain, and chat GPT. They emphasize the importance of automation and template-based functionality for simplifying the process of working with LLMs.'},\n",
       " {'title': 'Computer Vision Advances Render CAPTCHAs Obsolete',\n",
       "  'summary': 'Research shows that AI bots are more accurate than humans at identifying objects in images, rendering CAPTCHas obsolete. This has implications for website security and the ability to distinguish between human and robot users.',\n",
       "  'summary_unparsed': 'Computer Vision Advances Render CAPTCHAs Obsolete\\n\\n###SECTION SUMMARY###\\nResearch shows that AI bots are more accurate than humans at identifying objects in images, rendering CAPTCHas obsolete. This has implications for website security and the ability to distinguish between human and robot users.'},\n",
       " {'title': 'Podcast Discusses Vector Databases and Embedding Models for Generative AI Application',\n",
       "  'summary': 'This podcast segment discusses the importance of vector databases and embedding models in generative AI applications. The speakers highlight the challenges of finding relevant data and the need for efficient embedding models to handle large amounts of data. They also emphasize the importance of considering task-specific requirements when selecting an embedding model.',\n",
       "  'summary_unparsed': '\"Podcast Discusses Vector Databases and Embedding Models for Generative AI Applications\"\\n\\n###SECTION SUMMARY###\\nThis podcast segment discusses the importance of vector databases and embedding models in generative AI applications. The speakers highlight the challenges of finding relevant data and the need for efficient embedding models to handle large amounts of data. They also emphasize the importance of considering task-specific requirements when selecting an embedding model.'},\n",
       " {'title': 'Podcast Section: Vector Databases and Embeddings',\n",
       "  'summary': 'In this section, the speakers discuss the considerations and challenges of using vector databases and embeddings in AI applications. They touch on topics such as the trade-offs between GPU and CPU usage, the impact of embedding size on storage and performance, and the importance of evaluating best practices for specific use cases. Additionally, they highlight the dynamic nature of the field due to constant updates and improvements in AI technologies.',\n",
       "  'summary_unparsed': 'Podcast Section: Vector Databases and Embeddings\\n\\n###SECTION SUMMARY###\\nIn this section, the speakers discuss the considerations and challenges of using vector databases and embeddings in AI applications. They touch on topics such as the trade-offs between GPU and CPU usage, the impact of embedding size on storage and performance, and the importance of evaluating best practices for specific use cases. Additionally, they highlight the dynamic nature of the field due to constant updates and improvements in AI technologies.'},\n",
       " {'title': 'Model Middleware: Caching and Validation for Efficient AI Deployment',\n",
       "  'summary': 'This section discusses the importance of caching and validation in efficient AI deployments, specifically in the context of model middleware. The speakers highlight the benefits of caching, including reducing the number of requests made to databases and specialized hardware, and the potential for leveraging cached data to build a competitive moat. They also touch on the idea of validating inputs and outputs to ensure accuracy and prevent errors.',\n",
       "  'summary_unparsed': '\"Model Middleware: Caching and Validation for Efficient AI Deployments\"\\n\\n###SECTION SUMMARY###\\nThis section discusses the importance of caching and validation in efficient AI deployments, specifically in the context of model middleware. The speakers highlight the benefits of caching, including reducing the number of requests made to databases and specialized hardware, and the potential for leveraging cached data to build a competitive moat. They also touch on the idea of validating inputs and outputs to ensure accuracy and prevent errors.'},\n",
       " {'title': 'The Three Spokes of the AI Stack: Validation, Orchestration, and Model Hostin',\n",
       "  'summary': 'This section explores the three main components of the AI stack, including validation, orchestration, and model hosting. The speakers discuss the importance of considering these components when building an AI system and provide examples of how each component fits into the overall architecture.',\n",
       "  'summary_unparsed': '\"The Three Spokes of the AI Stack: Validation, Orchestration, and Model Hosting\"\\n\\n###SECTION SUMMARY###\\nThis section explores the three main components of the AI stack, including validation, orchestration, and model hosting. The speakers discuss the importance of considering these components when building an AI system and provide examples of how each component fits into the overall architecture.'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictfilt = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y) ])\n",
    "\n",
    "[dictfilt(topic, ('title','summary','summary_unparsed')) for topic in summarised_topics]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_whole_summary_prompt(summarised_topics):\n",
    "    summaries = '\\n\\n'.join([topic['summary'] for topic in summarised_topics])\n",
    "    prompt = f\"\"\"Written below are summaries of every topic of a podcast episode. Please write a detailed summary for the whole podcast episode.\n",
    "    \n",
    "###SECTION SUMMARIES###\n",
    "{summaries}\n",
    "\n",
    "###WHOLE SUMMARY###\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def summarise_transcript(transcript, llm):\n",
    "    prompt = get_whole_summary_prompt(transcript)\n",
    "    print(len(llm.tokenizer.tokenize(prompt)))\n",
    "    llm.model.config.max_new_tokens = 2048\n",
    "    summary = llm(prompt)[0]['generated_text']\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This podcast episode features conversations with several guests who are experts in their respective fields, all centered around the theme of computation and its impact on society. Topics discussed include the power of computational thinking, the limitations of human understanding, the potential of automated content selection, and the implications of advanced artificial intelligence on human consciousness. Guests include Stephen Wolfram, David Deutsch, and other notable figures in the fields of computer science, physics, and philosophy.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarised_transcript = summarise_transcript(summarised_topics, llm)\n",
    "summarised_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def summarise(transcript_split):\n",
    "    llm = load_llm_pipeline(cache_dir=\"/home/steph/.cache/huggingface/os_models\")\n",
    "    summarised_topics = summarise_topics(transcript_split, llm)\n",
    "    summarised_transcript = summarise_transcript(summarised_topics, llm)\n",
    "    return {\n",
    "        'summary': summarised_transcript, 'topics': summarised_topics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/podcast/practical_ai_236_tech_stack/transcript.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'summary': summarised_transcript, 'topics': summarised_topics\n",
    "    }, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE: This section is no longer required after the Llama2 release, which allows for a context length of 4096. This is a large enough context length for each topic section. All that is required is to have the topics staying below the context lengths."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics can end up being longer than the maximum context length of these models (2048 tokens). The options around this are either reducing the size of the topics (reasonable), or splitting them up, and doing a spit summarisation method. \n",
    "\n",
    "Langchain recommends a pattern which involves splitting the text up into parts, doing a summarisation for each of the parts, and taking these outputs to do the final summarisation. Lets try that and see how well it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3031, 2928, 2931, 3180, 2939, 2873, 2491, 2228, 2557, 2788, 3282, 2279, 3156, 3631, 3458]\n"
     ]
    }
   ],
   "source": [
    "print([len(llm.tokenizer.tokenize(topic['text'])) for topic in transcript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3364756104849838, 1.3854103343465045)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_word_ratio = [(len(llm.tokenizer.tokenize(topic['text']))/len(topic['text'].split(' '))) for topic in transcript ]\n",
    "np.mean(token_word_ratio), np.max(token_word_ratio)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we want to make sure that each topic stays under 4096/1.4=2900 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_num_tokens(text, tokenizer): return len(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6041 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6041"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_text = ' '.join(topic['text'] for topic in transcript[:3])\n",
    "get_num_tokens(topic_text, tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there's an opportunity to split it on specific separators. We can use paragraphs, or in fact we could actually do topics. Maybe just depends on whatever is fastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_text = split_paragraphs_text(topic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def chunk_text(topic_text, chunk_size=4000):\n",
    "    paragraphs = split_paragraphs_text(topic_text).split(\"\\n\\n\")\n",
    "    split_idxs = [0]\n",
    "    current_length = 0\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        current_length += len(paragraph)\n",
    "        if current_length > chunk_size:\n",
    "            split_idxs.append(i+1)\n",
    "            current_length = 0\n",
    "    if len(split_idxs) > 1: split_idxs.pop()\n",
    "    chunks = []\n",
    "    for i, j in zip(split_idxs, split_idxs[1:]+[None]):\n",
    "        if i > 0: \n",
    "            if j:\n",
    "                chunks.append('\\n\\n'.join(paragraphs[i-1:j+1]))\n",
    "            else:\n",
    "                chunks.append('\\n\\n'.join(paragraphs[i-1:j]))\n",
    "        else:\n",
    "            if j:\n",
    "                chunks.append('\\n\\n'.join(paragraphs[i:j+1]))\n",
    "            else:\n",
    "                chunks.append('\\n\\n'.join(paragraphs[i:j]))\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1328, 1275]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Now, one of the most basic assumptions of economics is that constraints are bad, at least in classical economics. People are always better off, or at least no worse off, when you relax constraints.\\n\\nSo more money is better than less money, and more hours in the day would be better than fewer hours in the day. Do you think literature is an exception to that economic logic, that constraints really make it better? So I think there's some sweet spot, right? If you constrain everything, then you're trapped in a really rigid box. There's no room for you to be creative. With absolutely no constraints at all, you're out in the wilderness. But with a few simple constraints, like you might have in a poetic form, that doesn't stop you being creative. It spurs you to creativity. And so there's this great quote from the Irish poet, Paul Muldoon, where he said that poetic form is a straitjacket in the sense that straitjackets were a straitjacket for Houdini.\\n\\nThey give you something to push off from. They give you an impetus to be creative within the structure that you've got. I'm not sure how familiar you are with six-word stories, which are an extreme version of this constraint, stories that are told in their entirety in six words. And the most famous one, which is attributed probably falsely to Ernest Hemingway, is this one.\\n\\nI think it goes, for sale, baby shoes, never worn. Yeah, yeah, yeah. Which I at least find to be an extremely haunting and powerful and memorable story because it says so much with so little. For sale, baby shoes, never worn. Yeah, exactly. And the brevity of it is part of the greatness of that tiny story.\\n\\nChekhov, his short stories are some of the greatest pieces of literature. There are what you can tell in a few pages. And with that restriction, I'm not going to give myself a whole novel. I'm going to tell this beautiful jewel, this crystalline perfection in a few pages. That makes it better. These short stories would not be better. As novels, they are perfect as short stories. My Freak and I was co-authors, Stephen Dubner. He started to get obsessed with these six-word stories, and he was invited to be part of a book where they asked famous authors to write their own memoir in six words. And I still remember his. It was on the seventh word He rested. which I find interesting because it reflects Stephen Dubner's identity as a writer and a creator, and also his unusual relationship with the Old Testament, because he's someone who converted to Judaism. But there's also something, I don't know, sacrilegious to it, because he's saying on the seventh word he rested, which is somehow making this direct comparison between himself and God. So it's, I don't know, it just makes me think, which I guess is the best you can hope for in six words. Wow, exactly. Now I really like that because as you're hearing the six words and you don't get it till the very final one, there is no seventh word and here's why, that tiny little story's told you.\\n\\nwhy we've stopped at six as well. It's fantastic, that. I love it. Exactly. So the people who took this constrained idea to their extreme, there was a French group of writers. What were they called again? The Oulipo. It's a shortening of the first two letters of Ouvoir et Littérature Potentielle, so Workshop for Potential Literature. And the Hundred Trillion Poems were by Raymond Connaught. He was one of the Oulipians. And they were all about exploring ways of making potential new kinds of literature with different constraints, really, mostly mathematical in nature.\\n\\nBut the most famous one, probably, is Georges Perec, who wrote a whole novel called La Disparition, which doesn't contain a single letter E. And this is a really good example where the first question probably anyone would ask is, why do that? And that's a really important thing to address because if we're just making up constraints and rules for no reason and playing intellectual games with them.\\n\\nBut that's what he was doing, right? Well, but it's more than that. The novel without the letter E, that wasn't the first book to be written omitting a single letter. So there was an earlier one which didn't use the letter E in the 1920s, but no one's heard of it.\\n\\nAnd I think the reason no one's heard of it is because it doesn't do anything cool with that restriction. Yes, it cleverly avoids using the letter E. It's a lot of work. It's very hard to do, but there's nothing in the story that's relevant to that. Whereas Perec's book, called La Disparation, The Disappearance, and in English it's called A Void, which are both clever titles, the book itself is about something that's missing, something that's disappeared. And then there are clues in the text. The characters know something's not right with the world. They're looking for something. There's an encyclopedia with 26 volumes, but volume five is missing. There's a hospital ward where there's no patient in bed number five because the fifth letter of the alphabet is E. And there are further layers towards this, so they work out eventually the characters, what's going on. But if you think about Georges Perrec himself, lots of letter E's in his own name, he lost both his parents during the Second World War. In French, you can't say family, mother, father. You can't say those words without the letter E.\",\n",
       " \"But that's what he was doing, right? Well, but it's more than that. The novel without the letter E, that wasn't the first book to be written omitting a single letter. So there was an earlier one which didn't use the letter E in the 1920s, but no one's heard of it.\\n\\nAnd I think the reason no one's heard of it is because it doesn't do anything cool with that restriction. Yes, it cleverly avoids using the letter E. It's a lot of work. It's very hard to do, but there's nothing in the story that's relevant to that. Whereas Perec's book, called La Disparation, The Disappearance, and in English it's called A Void, which are both clever titles, the book itself is about something that's missing, something that's disappeared. And then there are clues in the text. The characters know something's not right with the world. They're looking for something. There's an encyclopedia with 26 volumes, but volume five is missing. There's a hospital ward where there's no patient in bed number five because the fifth letter of the alphabet is E. And there are further layers towards this, so they work out eventually the characters, what's going on. But if you think about Georges Perrec himself, lots of letter E's in his own name, he lost both his parents during the Second World War. In French, you can't say family, mother, father. You can't say those words without the letter E.\\n\\nPerret cannot say his own name without the letter E. So this novel, which is about absence, disappearance, it has echoes within his own life around loss and things not being there. So that, for me, is what makes the use of a constraint interesting and worth doing if you actually do something with it's not just a random choice and that's exactly what we do in mathematics we don't just randomly think of rules we say okay these things seem to be what's happening in say geometry so you can set up some basic rules like we do in geometry at school this is what a line is this is what a point is this is what a circle is You need a starting point in mathematics. You don't choose the starting points randomly, otherwise it too would be just a sterile game. Mathematics is not that. Mathematics can help us understand so much because we don't do things at random.\\n\\nWe choose what our constraints are going to be, and then we play in a beautiful, wonderful playground of mathematics. So that is interesting what Perret did by leaving out the E. But then he wrote another book where he only used E's. Did he have a good story for that or was he just having fun? Well, he said it was all the E's that he hadn't used in La Disparation.\\n\\nIt was sort of lonely and sitting around waiting to be used. So, yeah, he wrote a book only with E's. And I think that was just, you know, having a bit of fun. And why not? We'll be right back with more of my conversation with mathematician Sarah Hart after this short break. So you masquerade as a normal person, but there's a point in your book where you reveal just how exceptionally weird and geeky you really are.\\n\\nDo you know what part of the book I'm talking about? It could be so many parts, Steve. There's one that jumped out. So there's something called fan fiction. And that's when regular people write stories involving popular fictional characters. But you propose something called fan-o-fiction. Yeah. Do you want to explain what fan-o-fiction is? So I was playing around with ideas around constraints, and when I mentioned this link between the constraints of poetry really giving us creative impetus to write beautiful sonnets or other kinds of poetic forms, and the constraints of geometry enabling us, those few little axioms that we start off geometry with, enabling us to prove beautiful theorems like Pythagoras's theorem. And so that got me to thinking, I wonder if there's a geometry-like constraint that I could play with and make a new form of potential fiction. It's a bit like your six-word stories. There is a kind of geometry, it's a particular mathematical structure called a projective geometry, where you only have a very few points. So, of course, in our normal world of geometry, there are infinitely many points. In this very strange structure called the Fano Plane, there are seven points and there are seven lines, and every line contains exactly three points.\\n\\nAnd it's got this beautiful structure. You can draw a little picture of this happening. It looks like a triangle with a circle inside it. And so what I did was to create a kind of literary form, which I'm just waiting for my Nobel Prize for literature here at this point. So points now become words, and lines are sentences. So the rules were, okay, there are only seven words that I can use. And I only allowed seven sentences. And every sentence has to contain exactly three of these words. So within that, I wrote this little story about some nonsense thing. But it was a really fun way to challenge myself to be creative, because if you've only got seven words and they've each got to appear in sentences, then some of those words have to do double duty as verbs and as nouns. So I think I had best as one of them. Best can be a verb. If you best someone, it means you come out top in a fight. Or it can be an adjective, right? The best show in the world, which is yours, of course.\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = chunk_text(transcript[1]['text'])\n",
    "print([get_num_tokens(text_chunk, tokenizer) for text_chunk in text_chunks])\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_chunk_summary_prompt(chunk_text):\n",
    "    return f\"\"\"Write a concise summary of the following:\n",
    "\n",
    "\\\"{chunk_text}\\\"\n",
    "\n",
    "CONCISE SUMMARY: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_chain_title_prompt(topic_text):\n",
    "    return f\"\"\"The following is a series of summaries of a text chapter. Generate a title from these summaries.\n",
    "\n",
    "It should be displayed in the below format:\n",
    "\n",
    "###CHAPTER SUMMARIES###\n",
    "summary of chapter describing why ai is good\n",
    "\n",
    "another summary of chapter describing why ai is good\n",
    "\n",
    "###CHAPTER TITLE###\n",
    "why ai is good\n",
    "\n",
    "Now try below:\n",
    "\n",
    "###CHAPTER SUMMARIES###\n",
    "{topic_text}\n",
    "\n",
    "###CHAPTER TITLE###\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_topic_title_chain(text, pipe):\n",
    "    text_chunks = chunk_text(text)\n",
    "    chunk_summaries = []\n",
    "    for text_chunk in text_chunks:\n",
    "        summary = pipe(get_chunk_summary_prompt(text_chunk))[0]['generated_text'].strip()\n",
    "        chunk_summaries.append(summary)\n",
    "    title = pipe(get_chain_title_prompt(\"\\n\\n\".join(chunk_summaries)))[0]['generated_text']\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = get_topic_title_chain(text_chunks, pipe)\n",
    "# title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1931"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 2048\n",
    "2048 - get_num_tokens(get_chain_title_prompt(\"\"), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def title_topics(topics, model, tokenizer):\n",
    "    pipe = pipeline(\n",
    "        model=model, tokenizer=tokenizer,\n",
    "        return_full_text=False,  \n",
    "        task='text-generation',\n",
    "        # we pass model parameters here too\n",
    "        # stopping_criteria=stopping_criteria\n",
    "        temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "        top_p=0.15,  # select from top tokens whose probability add up to 15%\n",
    "        top_k=0,  # select from top 0 tokens (because zero, relies on top_p)\n",
    "        max_new_tokens=480,  # max number of tokens to generate in the output\n",
    "        repetition_penalty=1.2  # without this output begins repeating\n",
    "    )\n",
    "    for topic in topics:\n",
    "        topic_text = topic['text']\n",
    "        if get_num_tokens(topic_text, tokenizer) < 1900: \n",
    "            topic['label'] = get_topic_title(topic_text, pipe)\n",
    "        else:\n",
    "            print(\"Topic is larger than the model's context window, running summary chain\", topic['label'])\n",
    "            topic['label'] = get_topic_title_chain(topic_text, pipe)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic is larger than the model's context window, running summary chain 1\n",
      "Topic is larger than the model's context window, running summary chain 2\n",
      "Topic is larger than the model's context window, running summary chain 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steph/.conda/envs/transcriber/lib/python3.10/site-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Exploring the Connections Between Literature and Mathematics', 'Constraints in Literature', '* Interactive Storytelling Through Graph Theory', 'The Power of Math in Fiction', 'Embracing Uncertainty in Mathematics Education', 'The Limits of Science in Economics']\n"
     ]
    }
   ],
   "source": [
    "transcript_titled = title_topics(transcript, model, tokenizer)\n",
    "print([ topic['label'] for topic in transcript_titled ])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/podcast/practical_ai_236_tech_stack/transcript.json\", 'w') as f:\n",
    "    json.dump(topics, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.], device='cuda:0', dtype=torch.float16),\n",
       " tensor([ 2660., 29904.], device='cuda:0', dtype=torch.float16),\n",
       " tensor([ 5204., 29904.], device='cuda:0', dtype=torch.float16),\n",
       " tensor([29936., 29936., 29936.], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0., 0.], device='cuda:0', dtype=torch.float16),\n",
       " tensor([ 9424., 29904.], device='cuda:0', dtype=torch.float16)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(x) for x in [\n",
    "        [''], ['User', ':'], ['system', ':'], ['#','#','#'], ['\\n', '\\n'],\n",
    "        [tokenizer.convert_ids_to_tokens([9427])[0], ':']\n",
    "    ]\n",
    "]\n",
    "stop_token_ids = [torch.LongTensor(x).to(device, dtype=torch.float16) for x in stop_token_ids]\n",
    "\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_string = \"\"\"Below is a transcript from a topic of a podcast delimited by triple backquotes.\n",
    "Please write an appropriate title for this topic in seven words or less.\n",
    "'''{text}'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_string = \"You are an AI language model designed to read podcast transcript sections and provide chapter titles for them. These chapter titles are to accurately summarise the text in 10 words or less. You are to provide such a title for the below piece of text:\\n\\n'''{text}'''\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_string =\"\"\"You are an AI language model designed to summarise podcast transcript chapters as chapter headings.\n",
    "\n",
    "For example:\n",
    "###CHAPTER TEXT###\n",
    "\\\"\\\"\\\"example text...\\\"\\\"\\\"\n",
    "###CHAPTER HEADING###\n",
    "\\\"\\\"\\\"Example Title\\\"\\\"\\\"\n",
    "\n",
    "Now try it:\n",
    "###CHAPTER TEXT###\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\n",
    "###CHAPTER HEADING###\n",
    "\\\"\\\"\\\"\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
