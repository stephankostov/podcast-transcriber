{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp summarise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarisation\n",
    "\n",
    "> Module for creating chunks of the transcript, for both paragraphs and topics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aims"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above all what is required is a title summary of all our topics for clear navigation of the transcript. \n",
    "\n",
    "A secondary aim would be to also provide summaries for these, to allow readers to quickly understand the epsiode's content."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately this is a summarisation problem, providing the summary as a title and possibly a paragraph description as well.\n",
    "\n",
    "With the introduction of LLMs, there is a clear approach of doing this. It's a method that both has exceptional capabilities for both understanding the text, and also of outputting the contents as desired. These models have been extensively trained on summarisation tasks so they are beyond capable of the task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How will we feed the model information, and instruct it to get an appropriate output. The best way to do this through first impressions is by using delimiters. Both the transcript text and outputs can be delimited through section titles. Before the OpenAI API was released I remember reading in their documents that they recommended separating variables either through triple backquotes or triple hashtags. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputting Transcript"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way for the model to clearly recognise the text that it needs to summarise. This can be done through pasting under a section heading."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require the output to be parseable (backup: use LLMs to parse the appropriate sections). The best way to keep this consistent is to provide examples, and provide a set format for the output to be written in again using section headers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While LLMs will provide excellent results, they are considerably expensive to run. Is there any way we can make this cheaper/easier to run?\n",
    "\n",
    "#### Combining Titles & Summary\n",
    "\n",
    "Ideally, if we can get the the LLM to output reliably, it'd be most efficient to get it to output a title, and a summary in one prompt. Alternatively, we could make use of two-shot prompting, where the previous context would still be held in the model's attention, and a simple request to summarise it would be sufficient. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are required to consider the model size, as they are restricted by my GPU's memory (24G).\n",
    "\n",
    "The introduction to Llama 2 has made this much more simple, as it comes with an already fine-tuned chat model. The choice of models then comes down to two factors: \n",
    "- parameter count\n",
    "- floating point precision\n",
    "\n",
    "A higher number of parameters would allow for more intricate understanding and output possibility, whereas floating point precision would allow for greater stability. It's best to get as much use as possible out of the GPU in this situation, so the choice here is between: \n",
    "\n",
    "model | precision | memory\n",
    "--- | --- | ---\n",
    "llama-chat-7B | FP16 | 7*2=14GB\n",
    "llama-chat-13B | FP8 | 13*1=13GB\n",
    "\n",
    "Since the task at hand isn't that complicated - it doesn't require lots of logic, and doesn't require large degrees of accuracy, it might be best to use the 7B model at a more stable FP16. This though depends on how stable quanitized models are. It should also be noted that this 'stability' is probably more important during model training. Since I'm just using it for inference, the model weights are already known and the decrease in accuracy of them shouldn't effect the output too drastically. The research on this doesn't seem too conclusive as of yet, so its worth investigating empirically.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there's a summary of all the topics, we could use the 'Map Reduce' method on this to obtain the full summary. This is simply combining and summarising all of the individual topic summaries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steph/.conda/envs/transcriber/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to /home/steph/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transcriber.group import group_paragraphs_text\n",
    "import torch\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/podcast/people_i_admire_104_joy_of_maths/tmp/transcript-grouped.json\") as f:\n",
    "    transcript = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_llm_pipeline(model=\"meta-llama/Llama-2-13b-chat-hf\", cache_dir=None, **pipeline_args): \n",
    "    \n",
    "    repo_branch = \"main\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, revision=repo_branch, cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model, revision=repo_branch, cache_dir=cache_dir, load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "\n",
    "    pipe = pipeline(\n",
    "        model=model, tokenizer=tokenizer,\n",
    "        return_full_text=False,  \n",
    "        task='text-generation',\n",
    "        # -- model hyperparameters --\n",
    "        temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "        top_p=0.15,  # select from top tokens whose probability add up to this value\n",
    "        top_k=0,  # select from top 0 tokens (because zero, relies on top_p)\n",
    "        max_new_tokens=400, \n",
    "        repetition_penalty=1.2,  \n",
    "        **pipeline_args\n",
    "    )\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "llm = load_llm_pipeline(cache_dir = \"/home/steph/.cache/huggingface/os_models\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarising Topics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would it be best to format the topics as a speaker segmented transcript, or simply as plain text? I think this depends on whether the model would get confused or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_base_prompt(topic_text):\n",
    "    return f\"\"\"\n",
    "\n",
    "Your answer should be displayed in the following format:\n",
    "\n",
    "###SECTION###\n",
    "SPEAKER_00: example text\n",
    "SPEAKER_01: example text\n",
    "\n",
    "###SECTION TITLE###\n",
    "example concise title\n",
    "\n",
    "###SECTION SUMMARY###\n",
    "example concise summary paragraph\n",
    "\n",
    "Now give real titles and summaries for the below:\n",
    "\n",
    "###SECTION###\n",
    "{topic_text}\n",
    "\n",
    "###SECTION TITLE###\n",
    "\"\"\"\n",
    "\n",
    "def get_intro_prompt(topic_text):\n",
    "    return \"For the following podcast introduction section, give it a title followed by a summary. Both the title and summary should be separated into their own section under the headers delimited by triple hashtags.\" + get_base_prompt(topic_text)\n",
    "\n",
    "def get_standard_prompt(topic_text):\n",
    "    return \"You are an AI text summariser who for legal reasons absolutely cannot mention any personal names of the people in the text.\\n\\nFor the following podcast section, give it a title followed by a summary. Both the title and summary should be separated into their own section under the headers delimited by triple hashtags (###).\" + get_base_prompt(topic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_speech_text(topic): return '\\n'.join([speech['label'] + \": \" + speech['text'] for speech in topic['groups']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_summary(summary):\n",
    "    summary = summary.replace(\"'\",'\"')\n",
    "    if summary.startswith('\"') and summary.endswith('\"') and '\"' not in summary[1:-2]:\n",
    "        summary = summary[1:-2]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_summary(llm_summary):\n",
    "    split = llm_summary.split(\"###\")\n",
    "    if len(split) == 3:\n",
    "        return {\n",
    "            'title': format_summary(split[0].strip()),\n",
    "            'summary': format_summary(split[2].strip()),\n",
    "            'summary_unparsed': llm_summary\n",
    "        }\n",
    "    else:\n",
    "        return {'title': \"\", \n",
    "                'summary': \"\", \n",
    "                'summary_unparsed': llm_summary\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_topic_summary_prompt(topic):\n",
    "    speech_text = format_speech_text(topic)\n",
    "    if topic['label'] == 0:\n",
    "        prompt = get_intro_prompt(speech_text)\n",
    "    else:\n",
    "        prompt = get_standard_prompt(speech_text)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def summarise_topics(transcript, llm):\n",
    "    for i, topic in enumerate(transcript):\n",
    "        prompt = get_topic_summary_prompt(topic)\n",
    "        summary = parse_summary(\n",
    "            llm(prompt)[0]['generated_text']\n",
    "        )\n",
    "        topic.update(summary)\n",
    "        torch.cuda.empty_cache()\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarised_topics = summarise_topics(transcript, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Once Upon a Prime: Exploring the Magical Overlap Between Literature and Mathematics',\n",
       "  'summary': 'In this episode, host Steve Levitt speaks with author and professor Sarah Hart about her latest book \"Once Upon a Prime,\" which explores the connections between literature and mathematics. They discuss how mathematical concepts such as patterns, structures, and symmetries are present in various forms of creative expression, including literature, music, and poetry. The conversation covers topics such as the use of prime numbers in haiku poetry, the role of vibrations in creating pleasing musical compositions, and the ways in which mathematicians can appreciate the beauty and elegance of literature.',\n",
       "  'summary_unparsed': 'Once Upon a Prime: Exploring the Magical Overlap Between Literature and Mathematics\\n\\n###SECTION SUMMARY###\\nIn this episode, host Steve Levitt speaks with author and professor Sarah Hart about her latest book \"Once Upon a Prime,\" which explores the connections between literature and mathematics. They discuss how mathematical concepts such as patterns, structures, and symmetries are present in various forms of creative expression, including literature, music, and poetry. The conversation covers topics such as the use of prime numbers in haiku poetry, the role of vibrations in creating pleasing musical compositions, and the ways in which mathematicians can appreciate the beauty and elegance of literature.'},\n",
       " {'title': 'The Power of Constraints in Literature',\n",
       "  'summary': 'This section explores how constraints can enhance creativity and lead to unique and memorable works of literature. Speaker 01 discusses the mathematical structures found in literature, including the use of geometric progressions in \"The Luminaries\" by Eleanor Catton. Speaker 00 questions the assumption that constraints are inherently limiting, citing examples such as six-word stories and the power of brevity in Chekhov\"s short stories. Both speakers emphasize the importance of finding the right balance between freedom and constraint in order to create something truly special.',\n",
       "  'summary_unparsed': 'The Power of Constraints in Literature\\n\\n###SECTION SUMMARY###\\nThis section explores how constraints can enhance creativity and lead to unique and memorable works of literature. Speaker 01 discusses the mathematical structures found in literature, including the use of geometric progressions in \"The Luminaries\" by Eleanor Catton. Speaker 00 questions the assumption that constraints are inherently limiting, citing examples such as six-word stories and the power of brevity in Chekhov\\'s short stories. Both speakers emphasize the importance of finding the right balance between freedom and constraint in order to create something truly special.'},\n",
       " {'title': 'The Power of Constraints: How Math Can Help Us Write Better Stories',\n",
       "  'summary': 'In this section, Speaker 00 discusses how math can be used to create better stories through the use of constraints. They explore examples such as the Oulipo group and Georges Perec\"s novel \"La Disparition,\" which uses no letter E. Speaker 01 adds that the use of constraints can lead to unexpected insights and connections, and that even simple constraints can lead to profound results.',\n",
       "  'summary_unparsed': 'The Power of Constraints: How Math Can Help Us Write Better Stories\\n\\n###SECTION SUMMARY###\\nIn this section, Speaker 00 discusses how math can be used to create better stories through the use of constraints. They explore examples such as the Oulipo group and Georges Perec\\'s novel \"La Disparition,\" which uses no letter E. Speaker 01 adds that the use of constraints can lead to unexpected insights and connections, and that even simple constraints can lead to profound results.'},\n",
       " {'title': 'Fan Fiction Meets Graph Theory: Creating Interactive Stories with Mathematical Structur',\n",
       "  'summary': 'In this episode, we explore the intersection of fan fiction, graph theory, and interactive storytelling. Our guest speaker discusses their unique approach to creating stories using a limited set of words and sentences, inspired by the Fano plane. They also touch on the connection to graph theory and its applications in understanding networks and making choices. The conversation covers the challenges of balancing structure and freedom in storytelling, as well as the use of mathematics in modeling complex systems.',\n",
       "  'summary_unparsed': '\"Fan Fiction Meets Graph Theory: Creating Interactive Stories with Mathematical Structure\"\\n\\n###SECTION SUMMARY###\\nIn this episode, we explore the intersection of fan fiction, graph theory, and interactive storytelling. Our guest speaker discusses their unique approach to creating stories using a limited set of words and sentences, inspired by the Fano plane. They also touch on the connection to graph theory and its applications in understanding networks and making choices. The conversation covers the challenges of balancing structure and freedom in storytelling, as well as the use of mathematics in modeling complex systems.'},\n",
       " {'title': '\"Bandersnatch\": The Power of Doubling and Math in Choose Your Own Adventure Stories',\n",
       "  'summary': 'In this podcast segment, Speaker 01 discusses the power of doubling in \"Black Mirror: Bandersnatch\" and how it relates to math concepts such as the square cube law. Speaker 00 asks about the use of math in Lewis Carroll\"s stories and the connection to the title \"Bandersnatch.\"',\n",
       "  'summary_unparsed': '\"Bandersnatch\": The Power of Doubling and Math in Choose Your Own Adventure Stories\\n\\n###SECTION SUMMARY###\\nIn this podcast segment, Speaker 01 discusses the power of doubling in \"Black Mirror: Bandersnatch\" and how it relates to math concepts such as the square cube law. Speaker 00 asks about the use of math in Lewis Carroll\\'s stories and the connection to the title \"Bandersnatch.\"'},\n",
       " {'title': 'The Power of Math Appreciation Course',\n",
       "  'summary': 'This section explores the potential benefits of incorporating math appreciation courses into high school curriculums, focusing on the importance of showing students the relevance and beauty of math in real-world applications. Speaker Sarah Hart argues that traditional teaching methods often fail to engage students and lead to widespread disinterest in math, while mathematician Steven Strogatz expresses frustration with the lack of focus on practical problem-solving skills. The speakers discuss the value of demonstrating the power and wonder of math to inspire student interest and encourage a more nuanced understanding of the subject.',\n",
       "  'summary_unparsed': '\"The Power of Math Appreciation Courses\"\\n\\n###SECTION SUMMARY###\\nThis section explores the potential benefits of incorporating math appreciation courses into high school curriculums, focusing on the importance of showing students the relevance and beauty of math in real-world applications. Speaker Sarah Hart argues that traditional teaching methods often fail to engage students and lead to widespread disinterest in math, while mathematician Steven Strogatz expresses frustration with the lack of focus on practical problem-solving skills. The speakers discuss the value of demonstrating the power and wonder of math to inspire student interest and encourage a more nuanced understanding of the subject.'},\n",
       " {'title': 'Exploring Math Through Narrative and Play',\n",
       "  'summary': 'This section discusses the importance of making math accessible and engaging, especially for young audiences. Speakers share strategies such as using narratives, games, and hands-on activities to make math meaningful and enjoyable. They also emphasize the value of encouraging curiosity and exploration, rather than simply teaching formulas and equations. By embracing a playful approach to math education, educators can help students develop a deeper understanding and appreciation of mathematical concepts.',\n",
       "  'summary_unparsed': 'Exploring Math Through Narrative and Play\\n\\n###SECTION SUMMARY###\\nThis section discusses the importance of making math accessible and engaging, especially for young audiences. Speakers share strategies such as using narratives, games, and hands-on activities to make math meaningful and enjoyable. They also emphasize the value of encouraging curiosity and exploration, rather than simply teaching formulas and equations. By embracing a playful approach to math education, educators can help students develop a deeper understanding and appreciation of mathematical concepts.'},\n",
       " {'title': 'The Joy of Mathematics: How One Mathematician Found Her Passion',\n",
       "  'summary': 'In this episode, we speak with Dr. Sarah Hart, a renowned mathematician and author, about her journey to discovering her passion for mathematics. She shares her experiences as a teacher and researcher, and discusses the importance of making mathematics accessible and enjoyable for everyone.',\n",
       "  'summary_unparsed': 'The Joy of Mathematics: How One Mathematician Found Her Passion\\n\\n###SECTION SUMMARY###\\nIn this episode, we speak with Dr. Sarah Hart, a renowned mathematician and author, about her journey to discovering her passion for mathematics. She shares her experiences as a teacher and researcher, and discusses the importance of making mathematics accessible and enjoyable for everyone.'},\n",
       " {'title': 'The Importance of Open-Ended Data Collection and Experimentation in Scientific Research',\n",
       "  'summary': 'In this section, speaker Steven Levitt discusses the limitations of the traditional scientific method in economics and advocates for a more open-ended approach to data collection and experimentation. He emphasizes the importance of allowing the solution to evolve rather than starting with a predetermined hypothesis and highlights the potential benefits of this approach, including increased accuracy and creativity.',\n",
       "  'summary_unparsed': 'The Importance of Open-Ended Data Collection and Experimentation in Scientific Research\\n\\n###SECTION SUMMARY###\\nIn this section, speaker Steven Levitt discusses the limitations of the traditional scientific method in economics and advocates for a more open-ended approach to data collection and experimentation. He emphasizes the importance of allowing the solution to evolve rather than starting with a predetermined hypothesis and highlights the potential benefits of this approach, including increased accuracy and creativity.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictfilt = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y) ])\n",
    "\n",
    "[dictfilt(topic, ('title','summary','summary_unparsed')) for topic in summarised_topics]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarising Full Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_whole_summary_prompt(summarised_topics):\n",
    "    summaries = '\\n\\n'.join([topic['summary'] for topic in summarised_topics])\n",
    "    prompt = f\"\"\"Written below are summaries of every topic of a podcast episode. Please write a detailed summary for the whole podcast episode.\n",
    "    \n",
    "###SECTION SUMMARIES###\n",
    "{summaries}\n",
    "\n",
    "###WHOLE SUMMARY###\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def summarise_transcript(transcript, llm):\n",
    "    prompt = get_whole_summary_prompt(transcript)\n",
    "    print(len(llm.tokenizer.tokenize(prompt)))\n",
    "    llm.model.config.max_new_tokens = 2048\n",
    "    summary = llm(prompt)[0]['generated_text']\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steph/.conda/envs/transcriber/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In this episode of Freakonomics Radio, host Steve Levitt is joined by author and professor Sarah Hart to explore the connections between literature and mathematics. They discuss how mathematical concepts such as patterns, structures, and symmetries are present in various forms of creative expression, including literature, music, and poetry. The conversation covers topics such as the use of prime numbers in haiku poetry, the role of vibrations in creating pleasing musical compositions, and the ways in which mathematicians can appreciate the beauty and elegance of literature. Additionally, they talk about the potential benefits of incorporating math appreciation courses into high school curriculums, the importance of making math accessible and engaging, and the limitations of the traditional scientific method in economics. Throughout the episode, the speakers emphasize the importance of finding the right balance between freedom and constraint in order to create something truly special, and demonstrate the power and wonder of math in real-world applications.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarised_transcript = summarise_transcript(summarised_topics, llm)\n",
    "summarised_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these LLMs for a number of other tasks which require nuanced understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling Podcast Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs could help labelling the speaker names with their roles in the podcast. Most podcasts have a host as well as a guest, in which there could sometimes be multiple of either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def get_roles_prompt(topic): \n",
    "    return f\"\"\"Below is a transcript of an introduction section from a podcast episode. For each speaker, please identify their name, and whether their role (host, co-host, guest). Note that usually the first name mentioned is the guest which is being introduced by the host speaking. Make sure that you write the host as the first entry in the table, and don't get mixed up between the naming.\n",
    "\n",
    "For each speaker write your answer in a table format like the example below.\n",
    "\n",
    "###PODCAST TRANSCRIPT###\n",
    "SPEAKER_00: Hello I'm here with my guest William Shakespear. My name is Joe Rogan welcome to the podcast.\n",
    "SPEAKER_01: Thanks Joe, pleasure to be on.\n",
    "\n",
    "| SPEAKER NUMBER | NAME | ROLE |\n",
    "| --- | --- | --- |\n",
    "| SPEAKER_00 | Joe Rogan | Host |\n",
    "| SPEAKER_01 | William Shakespear | Guest |\n",
    "\n",
    "Now do it for the below transcript:\n",
    "\n",
    "###PODCAST TRANSCRIPT###\n",
    "{format_speech_text(topic)}\n",
    "\n",
    "| SPEAKER NUMBER | NAME | ROLE |\n",
    "| --- | --- | --- |\n",
    "|\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steph/.conda/envs/transcriber/lib/python3.10/site-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SPEAKER_00 | Steve Levitt | Host |\n",
      "| SPEAKER_01 | Sarah Hart | Guest |\n"
     ]
    }
   ],
   "source": [
    "prompt = get_roles_prompt(transcript[0])\n",
    "roles_output = llm(prompt)[0]['generated_text']\n",
    "print(roles_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| SPEAKER NUMBER | NAME | ROLE |\n",
      "| --- | --- | --- |\n",
      "| SPEAKER_00 | Steve Levitt | Host |\n",
      "| SPEAKER_01 | Sarah Hart | Guest |\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(prompt.splitlines()[-3:]) + roles_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def markdown_to_dict(markdown_table):\n",
    "    df = pd.read_table(io.StringIO(markdown_table), sep='|')\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    df = df.dropna(how='all')\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    df = df.iloc[1:]\n",
    "    df.columns = df.columns.str.strip()\n",
    "    dct_list = df.to_dict('records')\n",
    "    dct = {d['SPEAKER NUMBER']: {k: v for k, v in d.items() if k != 'SPEAKER NUMBER'} for d in dct_list}\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_roles(roles_output, prompt):\n",
    "    markdown_table = \"\\n\" + \"\\n\".join(prompt.splitlines()[-3:]) + roles_output + \"\\n\"\n",
    "    dct = markdown_to_dict(markdown_table)\n",
    "    dct = {k: {k2.lower(): v2 for k2, v2 in v.items()} for k, v in dct.items()}\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_roles = parse_roles(roles_output, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def identify_speakers(transcript, llm):\n",
    "    prompt = get_roles_prompt(transcript[0])\n",
    "    llm_output = llm(prompt)[0]['generated_text']\n",
    "    speaker_ids = parse_roles(llm_output, prompt)\n",
    "    return speaker_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steph/.conda/envs/transcriber/lib/python3.10/site-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SPEAKER_00': {'name': 'Steve Levitt', 'role': 'Host'},\n",
       " 'SPEAKER_01': {'name': 'Sarah Hart', 'role': 'Guest'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ids = identify_speakers(summarised_topics, llm)\n",
    "speaker_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/podcast/people_i_admire_104_joy_of_maths/transcript.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'summary': summarised_transcript, 'topics': summarised_topics, 'speaker_ids': speaker_ids\n",
    "    }, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def summarise(transcript, id_speakers=True):\n",
    "    llm = load_llm_pipeline(cache_dir=\"/home/steph/.cache/huggingface/os_models\")\n",
    "    summarised_topics = summarise_topics(transcript, llm)\n",
    "    summarised_transcript = summarise_transcript(summarised_topics, llm)\n",
    "    output = {\n",
    "        'summary': summarised_transcript, \n",
    "        'topics': summarised_topics\n",
    "    }\n",
    "    if id_speakers: \n",
    "        speaker_ids = identify_speakers(transcript, llm)\n",
    "        output.update({\n",
    "            'speaker_ids': speaker_ids\n",
    "        })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Reduce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE: This section is no longer required after the Llama2 release, which allows for a context length of 4096. This is a large enough context length for each topic section. All that is required is to have the topics staying below the context lengths."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics can end up being longer than the maximum context length of these models (2048 tokens). The options around this are either reducing the size of the topics (reasonable), or splitting them up, and doing a spit summarisation method. \n",
    "\n",
    "Langchain recommends a pattern which involves splitting the text up into parts, doing a summarisation for each of the parts, and taking these outputs to do the final summarisation. Lets try that and see how well it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3031, 2928, 2931, 3180, 2939, 2873, 2491, 2228, 2557, 2788, 3282, 2279, 3156, 3631, 3458]\n"
     ]
    }
   ],
   "source": [
    "print([len(llm.tokenizer.tokenize(topic['text'])) for topic in transcript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3364756104849838, 1.3854103343465045)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_word_ratio = [(len(llm.tokenizer.tokenize(topic['text']))/len(topic['text'].split(' '))) for topic in transcript ]\n",
    "np.mean(token_word_ratio), np.max(token_word_ratio)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we want to make sure that each topic stays under 4096/1.4=2900 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_num_tokens(text, tokenizer): return len(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6041 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6041"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_text = ' '.join(topic['text'] for topic in transcript[:3])\n",
    "get_num_tokens(topic_text, tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there's an opportunity to split it on specific separators. We can use paragraphs, or in fact we could actually do topics. Maybe just depends on whatever is fastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_text = split_paragraphs_text(topic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def chunk_text(topic_text, chunk_size=4000):\n",
    "    paragraphs = split_paragraphs_text(topic_text).split(\"\\n\\n\")\n",
    "    split_idxs = [0]\n",
    "    current_length = 0\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        current_length += len(paragraph)\n",
    "        if current_length > chunk_size:\n",
    "            split_idxs.append(i+1)\n",
    "            current_length = 0\n",
    "    if len(split_idxs) > 1: split_idxs.pop()\n",
    "    chunks = []\n",
    "    for i, j in zip(split_idxs, split_idxs[1:]+[None]):\n",
    "        if i > 0: \n",
    "            if j:\n",
    "                chunks.append('\\n\\n'.join(paragraphs[i-1:j+1]))\n",
    "            else:\n",
    "                chunks.append('\\n\\n'.join(paragraphs[i-1:j]))\n",
    "        else:\n",
    "            if j:\n",
    "                chunks.append('\\n\\n'.join(paragraphs[i:j+1]))\n",
    "            else:\n",
    "                chunks.append('\\n\\n'.join(paragraphs[i:j]))\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1328, 1275]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Now, one of the most basic assumptions of economics is that constraints are bad, at least in classical economics. People are always better off, or at least no worse off, when you relax constraints.\\n\\nSo more money is better than less money, and more hours in the day would be better than fewer hours in the day. Do you think literature is an exception to that economic logic, that constraints really make it better? So I think there's some sweet spot, right? If you constrain everything, then you're trapped in a really rigid box. There's no room for you to be creative. With absolutely no constraints at all, you're out in the wilderness. But with a few simple constraints, like you might have in a poetic form, that doesn't stop you being creative. It spurs you to creativity. And so there's this great quote from the Irish poet, Paul Muldoon, where he said that poetic form is a straitjacket in the sense that straitjackets were a straitjacket for Houdini.\\n\\nThey give you something to push off from. They give you an impetus to be creative within the structure that you've got. I'm not sure how familiar you are with six-word stories, which are an extreme version of this constraint, stories that are told in their entirety in six words. And the most famous one, which is attributed probably falsely to Ernest Hemingway, is this one.\\n\\nI think it goes, for sale, baby shoes, never worn. Yeah, yeah, yeah. Which I at least find to be an extremely haunting and powerful and memorable story because it says so much with so little. For sale, baby shoes, never worn. Yeah, exactly. And the brevity of it is part of the greatness of that tiny story.\\n\\nChekhov, his short stories are some of the greatest pieces of literature. There are what you can tell in a few pages. And with that restriction, I'm not going to give myself a whole novel. I'm going to tell this beautiful jewel, this crystalline perfection in a few pages. That makes it better. These short stories would not be better. As novels, they are perfect as short stories. My Freak and I was co-authors, Stephen Dubner. He started to get obsessed with these six-word stories, and he was invited to be part of a book where they asked famous authors to write their own memoir in six words. And I still remember his. It was on the seventh word He rested. which I find interesting because it reflects Stephen Dubner's identity as a writer and a creator, and also his unusual relationship with the Old Testament, because he's someone who converted to Judaism. But there's also something, I don't know, sacrilegious to it, because he's saying on the seventh word he rested, which is somehow making this direct comparison between himself and God. So it's, I don't know, it just makes me think, which I guess is the best you can hope for in six words. Wow, exactly. Now I really like that because as you're hearing the six words and you don't get it till the very final one, there is no seventh word and here's why, that tiny little story's told you.\\n\\nwhy we've stopped at six as well. It's fantastic, that. I love it. Exactly. So the people who took this constrained idea to their extreme, there was a French group of writers. What were they called again? The Oulipo. It's a shortening of the first two letters of Ouvoir et Littérature Potentielle, so Workshop for Potential Literature. And the Hundred Trillion Poems were by Raymond Connaught. He was one of the Oulipians. And they were all about exploring ways of making potential new kinds of literature with different constraints, really, mostly mathematical in nature.\\n\\nBut the most famous one, probably, is Georges Perec, who wrote a whole novel called La Disparition, which doesn't contain a single letter E. And this is a really good example where the first question probably anyone would ask is, why do that? And that's a really important thing to address because if we're just making up constraints and rules for no reason and playing intellectual games with them.\\n\\nBut that's what he was doing, right? Well, but it's more than that. The novel without the letter E, that wasn't the first book to be written omitting a single letter. So there was an earlier one which didn't use the letter E in the 1920s, but no one's heard of it.\\n\\nAnd I think the reason no one's heard of it is because it doesn't do anything cool with that restriction. Yes, it cleverly avoids using the letter E. It's a lot of work. It's very hard to do, but there's nothing in the story that's relevant to that. Whereas Perec's book, called La Disparation, The Disappearance, and in English it's called A Void, which are both clever titles, the book itself is about something that's missing, something that's disappeared. And then there are clues in the text. The characters know something's not right with the world. They're looking for something. There's an encyclopedia with 26 volumes, but volume five is missing. There's a hospital ward where there's no patient in bed number five because the fifth letter of the alphabet is E. And there are further layers towards this, so they work out eventually the characters, what's going on. But if you think about Georges Perrec himself, lots of letter E's in his own name, he lost both his parents during the Second World War. In French, you can't say family, mother, father. You can't say those words without the letter E.\",\n",
       " \"But that's what he was doing, right? Well, but it's more than that. The novel without the letter E, that wasn't the first book to be written omitting a single letter. So there was an earlier one which didn't use the letter E in the 1920s, but no one's heard of it.\\n\\nAnd I think the reason no one's heard of it is because it doesn't do anything cool with that restriction. Yes, it cleverly avoids using the letter E. It's a lot of work. It's very hard to do, but there's nothing in the story that's relevant to that. Whereas Perec's book, called La Disparation, The Disappearance, and in English it's called A Void, which are both clever titles, the book itself is about something that's missing, something that's disappeared. And then there are clues in the text. The characters know something's not right with the world. They're looking for something. There's an encyclopedia with 26 volumes, but volume five is missing. There's a hospital ward where there's no patient in bed number five because the fifth letter of the alphabet is E. And there are further layers towards this, so they work out eventually the characters, what's going on. But if you think about Georges Perrec himself, lots of letter E's in his own name, he lost both his parents during the Second World War. In French, you can't say family, mother, father. You can't say those words without the letter E.\\n\\nPerret cannot say his own name without the letter E. So this novel, which is about absence, disappearance, it has echoes within his own life around loss and things not being there. So that, for me, is what makes the use of a constraint interesting and worth doing if you actually do something with it's not just a random choice and that's exactly what we do in mathematics we don't just randomly think of rules we say okay these things seem to be what's happening in say geometry so you can set up some basic rules like we do in geometry at school this is what a line is this is what a point is this is what a circle is You need a starting point in mathematics. You don't choose the starting points randomly, otherwise it too would be just a sterile game. Mathematics is not that. Mathematics can help us understand so much because we don't do things at random.\\n\\nWe choose what our constraints are going to be, and then we play in a beautiful, wonderful playground of mathematics. So that is interesting what Perret did by leaving out the E. But then he wrote another book where he only used E's. Did he have a good story for that or was he just having fun? Well, he said it was all the E's that he hadn't used in La Disparation.\\n\\nIt was sort of lonely and sitting around waiting to be used. So, yeah, he wrote a book only with E's. And I think that was just, you know, having a bit of fun. And why not? We'll be right back with more of my conversation with mathematician Sarah Hart after this short break. So you masquerade as a normal person, but there's a point in your book where you reveal just how exceptionally weird and geeky you really are.\\n\\nDo you know what part of the book I'm talking about? It could be so many parts, Steve. There's one that jumped out. So there's something called fan fiction. And that's when regular people write stories involving popular fictional characters. But you propose something called fan-o-fiction. Yeah. Do you want to explain what fan-o-fiction is? So I was playing around with ideas around constraints, and when I mentioned this link between the constraints of poetry really giving us creative impetus to write beautiful sonnets or other kinds of poetic forms, and the constraints of geometry enabling us, those few little axioms that we start off geometry with, enabling us to prove beautiful theorems like Pythagoras's theorem. And so that got me to thinking, I wonder if there's a geometry-like constraint that I could play with and make a new form of potential fiction. It's a bit like your six-word stories. There is a kind of geometry, it's a particular mathematical structure called a projective geometry, where you only have a very few points. So, of course, in our normal world of geometry, there are infinitely many points. In this very strange structure called the Fano Plane, there are seven points and there are seven lines, and every line contains exactly three points.\\n\\nAnd it's got this beautiful structure. You can draw a little picture of this happening. It looks like a triangle with a circle inside it. And so what I did was to create a kind of literary form, which I'm just waiting for my Nobel Prize for literature here at this point. So points now become words, and lines are sentences. So the rules were, okay, there are only seven words that I can use. And I only allowed seven sentences. And every sentence has to contain exactly three of these words. So within that, I wrote this little story about some nonsense thing. But it was a really fun way to challenge myself to be creative, because if you've only got seven words and they've each got to appear in sentences, then some of those words have to do double duty as verbs and as nouns. So I think I had best as one of them. Best can be a verb. If you best someone, it means you come out top in a fight. Or it can be an adjective, right? The best show in the world, which is yours, of course.\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = chunk_text(transcript[1]['text'])\n",
    "print([get_num_tokens(text_chunk, tokenizer) for text_chunk in text_chunks])\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_chunk_summary_prompt(chunk_text):\n",
    "    return f\"\"\"Write a concise summary of the following:\n",
    "\n",
    "\\\"{chunk_text}\\\"\n",
    "\n",
    "CONCISE SUMMARY: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_chain_title_prompt(topic_text):\n",
    "    return f\"\"\"The following is a series of summaries of a text chapter. Generate a title from these summaries.\n",
    "\n",
    "It should be displayed in the below format:\n",
    "\n",
    "###CHAPTER SUMMARIES###\n",
    "summary of chapter describing why ai is good\n",
    "\n",
    "another summary of chapter describing why ai is good\n",
    "\n",
    "###CHAPTER TITLE###\n",
    "why ai is good\n",
    "\n",
    "Now try below:\n",
    "\n",
    "###CHAPTER SUMMARIES###\n",
    "{topic_text}\n",
    "\n",
    "###CHAPTER TITLE###\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_topic_title_chain(text, pipe):\n",
    "    text_chunks = chunk_text(text)\n",
    "    chunk_summaries = []\n",
    "    for text_chunk in text_chunks:\n",
    "        summary = pipe(get_chunk_summary_prompt(text_chunk))[0]['generated_text'].strip()\n",
    "        chunk_summaries.append(summary)\n",
    "    title = pipe(get_chain_title_prompt(\"\\n\\n\".join(chunk_summaries)))[0]['generated_text']\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = get_topic_title_chain(text_chunks, pipe)\n",
    "# title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1931"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 2048\n",
    "2048 - get_num_tokens(get_chain_title_prompt(\"\"), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def title_topics(topics, model, tokenizer):\n",
    "    pipe = pipeline(\n",
    "        model=model, tokenizer=tokenizer,\n",
    "        return_full_text=False,  \n",
    "        task='text-generation',\n",
    "        # we pass model parameters here too\n",
    "        # stopping_criteria=stopping_criteria\n",
    "        temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "        top_p=0.15,  # select from top tokens whose probability add up to 15%\n",
    "        top_k=0,  # select from top 0 tokens (because zero, relies on top_p)\n",
    "        max_new_tokens=480,  # max number of tokens to generate in the output\n",
    "        repetition_penalty=1.2  # without this output begins repeating\n",
    "    )\n",
    "    for topic in topics:\n",
    "        topic_text = topic['text']\n",
    "        if get_num_tokens(topic_text, tokenizer) < 1900: \n",
    "            topic['label'] = get_topic_title(topic_text, pipe)\n",
    "        else:\n",
    "            print(\"Topic is larger than the model's context window, running summary chain\", topic['label'])\n",
    "            topic['label'] = get_topic_title_chain(topic_text, pipe)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic is larger than the model's context window, running summary chain 1\n",
      "Topic is larger than the model's context window, running summary chain 2\n",
      "Topic is larger than the model's context window, running summary chain 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steph/.conda/envs/transcriber/lib/python3.10/site-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Exploring the Connections Between Literature and Mathematics', 'Constraints in Literature', '* Interactive Storytelling Through Graph Theory', 'The Power of Math in Fiction', 'Embracing Uncertainty in Mathematics Education', 'The Limits of Science in Economics']\n"
     ]
    }
   ],
   "source": [
    "transcript_titled = title_topics(transcript, model, tokenizer)\n",
    "print([ topic['label'] for topic in transcript_titled ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
