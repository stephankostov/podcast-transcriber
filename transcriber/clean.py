# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/clean.ipynb.

# %% auto 0
__all__ = ['get_indexed_sentences', 'remove_punctuation', 'lower_case', 'remove_numbers', 'remove_stopwords', 'clean']

# %% ../nbs/clean.ipynb 2
import json
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk import download

# %% ../nbs/clean.ipynb 3
download('punkt')
download('stopwords')

# %% ../nbs/clean.ipynb 11
def get_indexed_sentences(text, min_length=10):
    sentences = []; sentence = []; index = 0
    for i, word in enumerate(text.split(' ')):
        sentence.append(word)
        word = word.replace('?', '.')
        if (word.endswith('.') or word.endswith('. ')) and len(sentence) > min_length: 
            sentences.append({
                'index': index,
                'text': ' '.join(sentence)
                })
            sentence = []
            index = i+1
    return sentences

# %% ../nbs/clean.ipynb 16
def remove_punctuation(text): #todo check if this removes any helpful punctuation (ie. individual characters such as "-")
    return text.translate(
        str.maketrans('', '', string.punctuation)
    )

def lower_case(text): return text.lower()

def remove_numbers(tokens): return [ word for word in tokens if word.isalpha() ]

def remove_stopwords(tokens):
    stop_words = set(stopwords.words('english'))
    return [ word for word in tokens if not word in stop_words ]

# %% ../nbs/clean.ipynb 17
def clean(text):

    # removing punctuation and upper cases
    text = lower_case(
        remove_punctuation(
            text
        ))
    
    # tokenisation
    tokens = word_tokenize(text)

    # removing stopwords and numeric tokens
    stop_words = set(stopwords.words('english'))
    tokens_cleaned = [
        token for token in tokens
        if token.isalpha() and token not in stop_words
    ]

    # lemmatizing words
    lemma = WordNetLemmatizer()
    lemmatized = [ lemma.lemmatize(token) for token in tokens_cleaned ]

    return " ".join(lemmatized)
