# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/group.ipynb.

# %% auto 0
__all__ = ['group_sentences', 'group_sentences_text', 'rev_sigmoid', 'activate_similarities', 'split_on_minimas',
           'get_abs_indexes', 'group_paragraphs', 'group_paragraphs_text', 'get_position_embeddings', 'get_embeddings',
           'get_topic_splits', 'group_topics', 'sentences_to_words', 'group_speakers', 'get_group_type', 'label_groups',
           'wrapped_partial', 'group_and_label', 'get_n_topics', 'group']

# %% ../nbs/group.ipynb 19
import pandas as pd
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from scipy.signal import argrelextrema
import math
from nltk.stem import WordNetLemmatizer
from nltk import download
from functools import partial, update_wrapper

# %% ../nbs/group.ipynb 21
download("wordnet")

# %% ../nbs/group.ipynb 24
def group_sentences(words, min_length=10):
    group_idxs = [0]
    for i, w in enumerate(words):
        w = w['word'].replace('?', '.')
        if (w.endswith('.') or w.endswith('. ')) and (i - (group_idxs[-1] + 1)) >= min_length: 
            group_idxs.append(i+1)
    if len(group_idxs) > 1:
        group_idxs.pop()
    sentences = [ words[i:j] for i, j in zip(group_idxs, group_idxs[1:]+[None]) ]
    return sentences, group_idxs

# %% ../nbs/group.ipynb 25
# wrapper around original function to be used for pure text
def group_sentences_text(text):
    words = [{"word": w} for w in text.split(" ")]
    sentences, _ = group_sentences(words)
    return sentences

# %% ../nbs/group.ipynb 32
def rev_sigmoid(x:float)->float:
    return (1 / (1 + math.exp(0.5*x)))
    
def activate_similarities(similarities:np.array, p_size=10)->np.array:
        """ Function returns list of weighted sums of activated sentence similarities
        Args:
            similarities (numpy array): it should square matrix where each sentence corresponds to another with cosine similarity
            p_size (int): number of sentences are used to calculate weighted sum 
        Returns:
            list: list of weighted sums
        """
        # To create weights for sigmoid function we first have to create space. P_size will determine number of sentences used and the size of weights vector.
        x = np.linspace(-10,10,p_size)
        # Then we need to apply activation function to the created space
        y = np.vectorize(rev_sigmoid) 
        # Because we only apply activation to p_size number of sentences we have to add zeros to neglect the effect of every additional sentence and to match the length ofvector we will multiply
        activation_weights = np.pad(y(x),(0,similarities.shape[0]-p_size))
        ### 1. Take each diagonal to the right of the main diagonal
        diagonals = [similarities.diagonal(each) for each in range(0,similarities.shape[0])]
        ### 2. Pad each diagonal by zeros at the end. Because each diagonal is different length we should pad it with zeros at the end
        diagonals = [np.pad(each, (0,similarities.shape[0]-len(each))) for each in diagonals]
        ### 3. Stack those diagonals into new matrix
        diagonals = np.stack(diagonals)
        ### 4. Apply activation weights to each row. Multiply similarities with our activation.
        diagonals = diagonals * activation_weights.reshape(-1,1)
        ### 5. Calculate the weighted sum of activated similarities
        activated_similarities = np.sum(diagonals, axis=0)
        return activated_similarities

# %% ../nbs/group.ipynb 35
def split_on_minimas(sentences, minimas):
    group_idxs = [ m for m in minimas[0] ]
    group_idxs.insert(0,0)
    paragraphs = [ sentences[i:j] for i, j in zip(group_idxs, group_idxs[1:]+[None]) ]
    return paragraphs, group_idxs

# %% ../nbs/group.ipynb 36
def get_abs_indexes(sentence_idxs, relative_indexes):
    return [ sentence_idxs[i] for i in relative_indexes ]

# %% ../nbs/group.ipynb 37
def group_paragraphs(sentences):
    if len(sentences) < 8: 
        return [sentences], [0]
    sentences_text = [ ' '.join([ word['word'] for word in s ]) for s in sentences ]
    embedding_model = SentenceTransformer('all-mpnet-base-v2')
    embeddings = embedding_model.encode(sentences_text)
    similarities = cosine_similarity(embeddings)
    activated_similarities = activate_similarities(similarities, p_size=8)
    minimas = argrelextrema(activated_similarities, np.less, order=2)
    paragraphs, group_idxs = split_on_minimas(sentences, minimas)
    return paragraphs, group_idxs

# %% ../nbs/group.ipynb 38
# wrapped around original function to be used with pure text as opposed to sentence grouped words
def group_paragraphs_text(text):
    words = [{"word": w} for w in text.split(" ")]
    sentences, _ = group_sentences(words)
    paragraphs, _ = group_paragraphs(sentences)
    paragraphs = '\n\n'.join([ ' '.join([  ' '.join([ word['word'] for word in sentence ]) for sentence in paragraph ]) for paragraph in paragraphs ])
    return paragraphs

# %% ../nbs/group.ipynb 51
def get_position_embeddings(seq_len, d, n=1000):
    P = np.zeros((seq_len, d))
    for k in range(seq_len):
        for i in np.arange(int(d/2)):
            denominator = np.power(n, 2*i/d)
            P[k, 2*i] = np.sin(k/denominator)
            P[k, 2*i+1] = np.cos(k/denominator)
    return P

# %% ../nbs/group.ipynb 55
def get_embeddings(sentences):
    sentence_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
    sentence_embeddings = sentence_model.encode(sentences)
    position_embeddings = get_position_embeddings(*sentence_embeddings.shape)
    position_embeddings = position_embeddings * (np.max(sentence_embeddings) / np.max(position_embeddings)) # scale to match sentence embeddings
    return sentence_embeddings + position_embeddings

# %% ../nbs/group.ipynb 58
from sklearn.cluster import KMeans

# %% ../nbs/group.ipynb 63
# splitting topics into consecutive topics 
def get_topic_splits(sentences_clustered):
    group_idxs = [0]
    for i in range(1, len(sentences_clustered)):
        if sentences_clustered[i][0] != sentences_clustered[i-1][0]: 
            group_idxs.append(i)
    return group_idxs

# %% ../nbs/group.ipynb 65
def group_topics(sentences, n_topics):
    sentences_text = [ ' '.join([ word['word'] for word in s ]) for s in sentences ]
    embeddings = get_embeddings(sentences_text)
    clusters = KMeans(n_clusters=n_topics).fit(embeddings)
    sentences_clustered = list(zip(clusters.labels_, sentences))
    group_idxs = get_topic_splits(sentences_clustered)
    topics = [ sentences[i:j] for i, j in zip(group_idxs, group_idxs[1:]+[None]) ]
    return topics, group_idxs

# %% ../nbs/group.ipynb 88
# flattening sentence array
def sentences_to_words(sentences):
    return [word for sentence in sentences for word in sentence]

# %% ../nbs/group.ipynb 89
# creating speech groups by consecutive speaker
def group_speakers(sentences):
    words = sentences_to_words(sentences)
    group_idxs = [0]
    for i in range(1, len(words)):
        if words[i]['speaker'] != words[i-1]['speaker']:
            group_idxs.append(i)
    word_groups = [ words[i:j] for i, j in zip(group_idxs, group_idxs[1:]+[None]) ]
    sentence_groups = [ group_sentences(words)[0] for words in word_groups ]
    return sentence_groups, group_idxs

# %% ../nbs/group.ipynb 92
def get_group_type(group_function):
    lemmatizer = WordNetLemmatizer()
    return lemmatizer.lemmatize(group_function.__name__.split("_")[1])

# %% ../nbs/group.ipynb 93
def label_groups(groups, group_type):
    word_label = True if group_type in groups[0][0][0] else False
    return [ {
            'type': group_type,
            'label': group[0][0][group_type] if word_label else i,
            'start': group[0][0]['start'],
            'end': group[-1][-1]['end'],
            'text': ' '.join([word['word'] for word in sentences_to_words(group)]),
            'groups': group
        } for i, group in enumerate(groups)
    ]

# %% ../nbs/group.ipynb 94
# warpper function to allow `function.__name__` attribute to be called for partial functions
def wrapped_partial(func, *args, **kwargs):
    partial_func = partial(func, *args, **kwargs)
    update_wrapper(partial_func, func)
    return partial_func

# %% ../nbs/group.ipynb 95
def group_and_label(sentences, split_functions):
    split_function = split_functions[0]
    groups, _ = split_function(sentences)
    groups = label_groups(groups, get_group_type(split_function))
    split_functions = split_functions[1:]
    if split_functions: # nest function if there are more groupings
        for i, group in enumerate(groups):
            groups[i]['groups'] = group_and_label(group['groups'], split_functions)
    else: # cleanup at the end
        for i, group in enumerate(groups):
            groups[i]['words'] = [word for sentence in groups[i]['groups'] for word in sentence]
            groups[i].pop('groups')
    return groups

# %% ../nbs/group.ipynb 105
def get_n_topics(words): return int(np.ceil(len(words)/2150))

# %% ../nbs/group.ipynb 107
def group(words, n_topics=None, split_functions=None):
    if not n_topics: n_topics = get_n_topics(words)
    sentences, _ = group_sentences(words) # functions standardised to take sentences as inputs, as this is what is required for topics & paragraphs
    split_functions = [wrapped_partial(group_topics, n_topics=n_topics), group_speakers, group_paragraphs] if not split_functions else split_functions
    transcript_split = group_and_label(sentences, split_functions)
    return transcript_split
